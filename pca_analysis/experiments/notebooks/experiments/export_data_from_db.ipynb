{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26eb47c8-7dc9-4002-a849-86cd5c0749ec",
   "metadata": {},
   "source": [
    "---\n",
    "cdt: 2024-09-04T15:56:31\n",
    "title: \"Migrating From DuckDB to NetCDF\"\n",
    "description: \"A migration from the DuckDB database to a new NetCDF file. Includes meta and chromatospectral data.\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ace095-92fb-4138-8b40-413c6779e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as db\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a5105-7827-4626-be74-4126b42291a2",
   "metadata": {},
   "source": [
    "To do:\n",
    "\n",
    "- [x] create output dir\n",
    "- [x] export cs data\n",
    "- [ ] create xarray format from dataarray\n",
    "- [ ] export metadata\n",
    "- [ ] join cs data to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8c3d9-6548-4351-afe9-cc2b13f89275",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/wines.db\"\n",
    "netcdf_path = \"/Users/jonathan/mres_thesis/netcdf\"\n",
    "csv_outpath = \"/Users/jonathan/mres_thesis/netcdf/csvs/cs.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01e0ad-8704-449f-a60c-cc4f1838ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = db.connect(db_path)\n",
    "\n",
    "metadata_colnames = con.sql(\"select table_schema, table_name, column_name from information_schema.columns where table_name ='sample_metadata' AND table_schema='pbl'\").df()['column_name'].to_list()\n",
    "\n",
    "query = \\\n",
    "f\"\"\"\n",
    "COPY\n",
    "    (\n",
    "    SELECT\n",
    "            *\n",
    "    FROM\n",
    "        chromatogram_spectra\n",
    "    JOIN\n",
    "        (\n",
    "            select\n",
    "                *\n",
    "            from\n",
    "                pbl.sample_metadata\n",
    "        )\n",
    "    USING\n",
    "         (id)\n",
    "    ORDER BY\n",
    "        id, mins\n",
    "    )\n",
    "TO\n",
    "    '/Users/jonathan/mres_thesis/netcdf/csvs/cs.csv'\n",
    "(FORMAT CSV);\n",
    "\"\"\"\n",
    "if not Path(csv_outpath).exists():\n",
    "    con.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5647c21-fbc2-4487-ab9b-758ab66685b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cs\n",
    "except NameError:\n",
    "    cs = pd.DataFrame()\n",
    "\n",
    "if cs.empty:\n",
    "    cs = pl.read_csv(csv_outpath, schema_overrides={'samplecode':str}).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7ac10-7131-48ae-8980-36502c638ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"show\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36f281-9328-45f8-b127-7a8032aa3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_query = \\\n",
    "\"\"\"\n",
    "select\n",
    "    count(*)\n",
    "from\n",
    "    chromatogram_spectra\n",
    "join\n",
    "    pbl.sample_metadata\n",
    "using\n",
    "    (id)\n",
    "where\n",
    "    id = '0aeed887-d8e9-4886-baac-f519c4f44715'\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "con.sql(size_query).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c00fa0-2d48-446d-9285-0ea9ea942a89",
   "metadata": {},
   "source": [
    "## Chunkwise DataFrame to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b50ad-b43c-4cf8-9272-6f3db39f0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims are the names of the dimensions, the axes\n",
    "# coordinates are the tick values of the dimensions\n",
    "# vars are the values that exist within the dims, labelled by the coordinates.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cs.columns = [col.replace(\"nm_\",\"\") for col in cs.columns]\n",
    "\n",
    "cs[metadata_colnames].drop_duplicates()\n",
    "\n",
    "cs_idxed = cs.set_index(['id','mins'])\n",
    "\n",
    "grpby_id = cs.groupby('id')\n",
    "\n",
    "ds_list = []\n",
    "\n",
    "curr_id = None\n",
    "\n",
    "group_sizes = []\n",
    "mean_grp_size = None\n",
    "\n",
    "for i, (k, v) in enumerate(grpby_id):\n",
    "\n",
    "    # check the group sizes against the progressive mean, if its an outlier, raise an alarm\n",
    "    rows = v.shape[0]\n",
    "    if i==0:\n",
    "        group_sizes.append(size)\n",
    "        mean_grp_size = np.mean(group_sizes)\n",
    "    else:\n",
    "        if size > mean_grp_size:\n",
    "            raise RuntimeError(f\"outlier size detected: {k}. {mean_grp_size=}, {size=}, \")\n",
    "        \n",
    "    metadata_dict = v[metadata_colnames].drop_duplicates().to_dict(orient='list')\n",
    "    id_vals = v['id'].values\n",
    "    min_vals = v[\"mins\"].values\n",
    "    min_vals = np.round(min_vals - min_vals[0], 6)\n",
    "    wavelength_vals = v.drop([\"id\",\"mins\"]+metadata_colnames,axis=1).columns.astype(int)\n",
    "\n",
    "    data = v.drop([\"id\",\"mins\"]+metadata_colnames, axis=1).values\n",
    "    \n",
    "    ds_list.append(xr.Dataset(\n",
    "        data_vars = {\n",
    "            \"abs\":(('mins','nm'),data), \n",
    "        },\n",
    "        coords = {\n",
    "            'mins': min_vals,\n",
    "            'nm': wavelength_vals,\n",
    "            'id': k,\n",
    "            **metadata_dict\n",
    "        }\n",
    "    ))\n",
    "\n",
    "display(ds_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9277311-6a4d-4e66-abbf-3ea8065549cb",
   "metadata": {},
   "source": [
    "Add the remaining metadata as coords. Do this by adding the keys to the dim tuple and unpack into the coords dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce27141-b563-4bd9-84a5-6ad781bb2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n_chunks):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    length = len(lst)\n",
    "    step_size = length/n_chunks\n",
    "    \n",
    "    assert (length/step_size).is_integer(), (length, step_size, x)\n",
    "    \n",
    "    start = int(step_size)\n",
    "    step = int(step_size)\n",
    "    end = length - step\n",
    "\n",
    "    print(\"start:\", start)\n",
    "    print(\"end:\",end)\n",
    "    print(\"step\",step)\n",
    "    \n",
    "    for idx, i_0 in enumerate(range(0, end, step)):\n",
    "        print(\"iteration: \",idx)\n",
    "        print(\"\\ti_0: \", i_0)\n",
    "\n",
    "        i_n = i_0 + int(step_size)\n",
    "\n",
    "        print(\"\\ti_n: \", i_n)\n",
    "        yield lst[i_0:i_n+1]\n",
    "\n",
    "chunked_ds = [xr.concat(chunk, dim='id') for chunk in chunks(ds_list,35)]\n",
    "chunked_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19fc662-a821-4fb9-95d9-c780c8917f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2 = [xr.concat(chunk, dim='id') for chunk in chunks(chunked_ds,3)]\n",
    "ds_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc83766-6e87-404d-baf0-666e943efd19",
   "metadata": {},
   "source": [
    "Alright, going to have to trim each mode down to the mean (wavelength and mins primarily) if I want to fit it all into one Dataset.\n",
    "\n",
    "The quicker thing right now would be to prepare the raw reds as a dataset and continue the decompositions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1b359-64e4-4f05-8a0c-60c8a85018be",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_ds[19]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f99564-ae8e-4bb8-bda3-d4385c346e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_2[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4442abb-e85c-4013-80ed-16ff399aa97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.concat(ds_2, dim='id')\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08193b96-f0de-4588-9424-736c210e806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(id=0)[['mins','abs']].to_dataarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e964d0-10a8-498d-83ca-021bdb213a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from <https://earth-env-data-science.github.io/lectures/xarray/xarray.html#datasets>, coords are constant values such as nm, mins.\n",
    "# they designate the space.\n",
    "# variables change. That would be for example absorbance.\n",
    "\n",
    "# break ds_list into 5 and concatenate each then concatenate the remainder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220e7d2-5066-4bdd-91b7-6a1bf2aa3b83",
   "metadata": {},
   "source": [
    "Ok so its clear that XArray wants to have all the dimensions be the same across the samples, bar the joining dimesnion (\"id\" in this case). We're going to need to perform some EDA on the SQL-stored data to identify ways of bringing all the data onto the same dimensions, and whether there should be more than one dataset. The main pain point will be data observed at different frequencies. See [Dataset EDA](dataset_description_wavelength_time.ipynb) for more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
