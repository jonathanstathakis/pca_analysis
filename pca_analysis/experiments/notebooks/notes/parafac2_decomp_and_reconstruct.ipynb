{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cceb1ac8-3fb8-4359-9f5e-9ab0ad618026",
   "metadata": {},
   "source": [
    "---\n",
    "cdt: 2024-09-03T14:21:28\n",
    "title: PARAFAC2 Decomposition Factor Matrices\n",
    "description: A brief description of how the PARAFAC2 model handles Chromatographic Data, and how to reconstruct the tensor\n",
    "notes:\n",
    "    - Am using tensor in a general sense as B is 3 way and the other factors are 2 way, authors tend to discuss everything as 2 way while addressing one slice of B.\n",
    "todo:\n",
    "    - add viz.\n",
    "---\n",
    "\n",
    "\n",
    "The PARAFAC2 decomposition model depicts a three (or more) way tensor $\\mathcal X \\in \\mathbb R^{I \\times J \\times K}$ as the product of four tensors, each containing the contribution of each of the modes to the character of $X$. Furthermore, to produce a correct fit, we need to estimate the number of latent factors, which correspond to the rank $R$. Through this decomposition we can isolate the $I$ from the influence of $J$, etc. We are then able to obtain the contribution of each mode to each factor, or vice versa. For a set of samples characterised through three-way chemical detection, such as liquid chromatography with a hyphenated two-way detector, we can consider the modes as follows: $I$ are the samples, $J$ is the elution time points, and $K$ is the spectral channels. To decompose this mixture we need to identify the correct $R$, which in this data corresponds to the number of peaks and a noise component. A naive method would be to simply count the number of observable peaks. The result of the decomposition can be expressed as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal X_{(I,J,K)} = A_{(K,R)} \\space P_{(K,I,R)} \\space B_{(R,R)} \\space C_{(J,R)}\n",
    "$$\n",
    "\n",
    "Where $A_{(K,R)}$, $B_{(R,R)}$, and $C_{(J,R)}$ are factor tensors, $P_{(K,I,R)}$ are the projection matrices (what is their significance, and what is B?). The product of $P_{(K, I, R)}$ and $B_{(R,R)}$ is more useful, and as such the decomposition is expressed as:\n",
    "\n",
    "$$\n",
    "\\mathcal X_{(I,J,K)} = A_{(K,R)} \\space B_{(K,I,R)} \\space C_{(J,R)}\n",
    "$$\n",
    "\n",
    "Where each element of $A$ is the $kth$ samples $rth$ factor (analyte) weight (concentration), each $kth$ slice of B contains a two way matrix of the elution profiles $I$ of each $rth$ factor, and $C$ contains the spectral profiles $J$ of each $rth$ factor.\n",
    "\n",
    "Simply put, $A$ contains the analyte concentrations for each sample in each column, $B$ is a three-way tensor where each slice is a sample and columns are elution profiles for each analyte, and $C$ is the analyte profiles over the spectrum. Put another way, $A$ stores the contribution of the analyte abundance on the tensor modes, then $B$ and $C$ are pure descriptions of how the analytes behave over time and spectral channel, respectively. Thus if you were interested only in the concentrations, you would use each row of $A$ for each sample, if it was the response to time, then $B$, or channel, $C$.\n",
    "\n",
    "Interpretation of $B$ is not intuitive as we are used to observing signals through a spectral channel, such as a wavelength of light. The columns of $B$ contain the response of the analytes over time *without* the spectral component, and are thus abstracted. Since we are generally interested in the separation of the members of a chemical mixture, sometimes dubbed *mathematical chromatography*, it is more intuitive to observe the profiles of $B$ with scaling from $A$, the abundance, and a row of $C$ a spectral channel. The calculation would be as follows for a $kth$ sample, $jth$ channel:\n",
    "\n",
    "$$\n",
    "B_{(k,I,R)}' = A_{(k,R)} \\odot B_{(k,I,R)} \\odot C_{(j,R)}\n",
    "$$\n",
    "\n",
    "Where $B_{(k,I,R)}'$ is the $kth$ matrix of $B{(K, I, R)}$, $A_{(k, R)}$ is the $kth$ row of $A_{(K,R)}$, $C_{(k, R)}$ is the $jth$ row of $C_{(j,R)}$ and $\\odot$ is the Hadamard, or element-wise product. $B_{(k,I,R}'$ can then be compared to the $kth$ sample at the same $j$ for fit evaluation. It can be seen from here how to reconstruct a full three-way tensor [@kiers_parafac2parti_1999; @tensorly]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832176ce-329f-4f90-9f5f-abdd730ed7df",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98403d4-f864-41ce-8ccf-f2c3bfc05503",
   "metadata": {},
   "source": [
    "Note: The TensorLy docs contain inconsistant formulation of the PARAFAC2 model. For example, In `Parafac2` [documentation](https://tensorly.org/stable/modules/generated/tensorly.decomposition.Parafac2.html) and the [demonstration](https://tensorly.org/stable/auto_examples/decomposition/plot_parafac2.html), where the demonstration, in agreement with the literature [@bro_parafac2partii_1999], uses $k$ for the sample, but the prior uses $i$.\n",
    "\n",
    "Not a huge deal but a trip hazard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
