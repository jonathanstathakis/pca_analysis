{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "cdt: 2024-12-10T09:53:38\n",
    "title: Clustering\n",
    "desc: \"notes on clustering of 2D (1D?) arrays\"\n",
    "conc: \"\"\n",
    "project: \"notes\"\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Clustering\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Intro\n",
    "\n",
    "- sklearn has clustering `sklearn.cluster`. This provides both class and function API's for each algorithm.\n",
    "- the algorithms can accept both data matrices (n_samples, n_features) and similarity matrices (n_samples, n_samples). Presumably to enable clustering mid-analysis, or chaining custom similarity metrics to clustering.\n",
    "- a.\n",
    "\n",
    "### Defining the Problem\n",
    "\n",
    "- [this stackoverflow post](https://stackoverflow.com/questions/11513484/1d-number-array-clustering) has comments which state that clustering shouldnt be used for 1D problems, and that for the these problems, the term isnt clustering, but rather *segmentation* or *natural breaks optimization*.  They state that Kernel Density Estimation is one method,  and Jenks Natural Breaks Optimization is another. They state that KDE is the most sound method. [this](https://stackoverflow.com/questions/35094454/how-would-one-use-kernel-density-estimation-as-a-1d-clustering-method-in-scikit/35151947#35151947) is an example of using KDE for 'clustering'.\n",
    "- Jenks is an algorithm for whom the original article is [unaccessible](https://macwright.com/2013/02/18/literate-jenks.html)\n",
    "\n",
    "\n",
    "### Heirarchical Agglometrative Clustering\n",
    "\n",
    "- https://scikit-learn.org/1.5/modules/clustering.html#hierarchical-clustering:\n",
    "    - Heirarchical Agglometrative Clustering (HAC) builds nested clusters\n",
    "    - the relationship between clusters is heirarchical and is modelled as a tree (dendogram)\n",
    "    - The root of the tree is the core of the cluster, and each leaf is a single sample.\n",
    "    - sklearn's `AggolmerativeClustering` uses a bottom up approach - first each sample is in its own cluster, th\n",
    "    en clusters are merged together according to a linkage criteria.\n",
    "    - possible linkage criteria:\n",
    "        - ward: minimizes sum of squared differences in all clusters. This is a variance minimizing approach.\n",
    "        - maximum or complete linkage: minimizes maximum distance between observations of pairs of clusters.\n",
    "        - average linkage: minimizes average of distance between all observations of pairs of clusters.\n",
    "        - single linkage: minimizes distance between closest observations of pairs of clusters.\n",
    "    - agglomerative clustering can be costly as each possible merge is considered in every iteration.\n",
    "    - agglomerative clustering can be sped up by using it with a connectivity matrix (?)\n",
    "    - agglomerative clustering can be described as 'rich get richer'. This leads to uneven cluster sizes with single linkage exhbiting the most of this characeristic and Ward producing the most even cluster sizes.\n",
    "    - Affinity is the measure of how far a cluster can extend.\n",
    "    - ward linkage criteria affinity can not be modified.\n",
    "    - for non-euclidean metrics, average linkage is a good alternative for ward (?).\n",
    "    - single linkage is not robust to noisy data but is efficient.\n",
    "    - single linkage performs well on non-globular data. See \"sklearn-clustering\"\n",
    "    - there is a method of plotting the dendogram.\n",
    "- It requires knowing the number of clusters. If the number of clusters are not known beforehand, Mean Shift, DBSCAN, OPTICS etc are recommended. See [this stackoverflow post](https://datascience.stackexchange.com/questions/20248/agglomerative-clustering-without-knowing-number-of-clusters) - this is not true, `distance_threshold` is mutually exclusive with `n_clusters`.\n",
    "- The order in which sklearn labels the clusters may not correspond to the input data order [see this post](https://stackoverflow.com/questions/56485763/are-the-labels-output-of-cluster-algorithms-ordered-in-a-certain-order-python).\n",
    "<div>\n",
    "<img src=\"./attachments/sphx_glr_plot_linkage_comparison_001.png\" width=\"500\"  />\n",
    "<figcaption>sklearn-clustering</figcaption>\n",
    "<a name=\"sklearn-clustering\"</a>\n",
    "<div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca-analysis-6KQS4gUX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
