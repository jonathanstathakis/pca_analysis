{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Create XArray Dataset\n",
    "description: \"create the raw dataset xarray (nc) file through `database_etl`\"\n",
    "project: parafac2\n",
    "conclusion: \"while execution of database_etl to produce the expected dataset was successful, we found that xarray datasets wernt queryable across variables as was expected. Efforts to recreate the img data within duckdb were fruitless, as expected, however we did find that judicious use of double quotes and string formatting enabled us to construct proper tidy tables with wavelength integer column labels, but unfortunately unidentified (probably memory ) problems prevented ingestion of data in this method to be practical as it would have taken 32 minutes if inserted row by row. Recommendation is to construct an intermediary between sql queries and the xarray dataset or the stored parquet files. Probably the latter as it removes one complication.\"\n",
    "status: closed\n",
    "cdt: 2024-09-25T16:17:38\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from database_etl import etl_pipeline_raw, get_data\n",
    "import polars as pl\n",
    "\n",
    "from pca_analysis.definitions import (\n",
    "    RAW_LIB_DIR,\n",
    "    DIRTY_ST,\n",
    "    CT_UN,\n",
    "    CT_PW,\n",
    "    DB_PATH_UV,\n",
    "    NC_RAW,\n",
    ")\n",
    "\n",
    "import duckdb as db\n",
    "import xarray as xr\n",
    "\n",
    "con = db.connect(DB_PATH_UV)\n",
    "con.close()\n",
    "overwrite = True\n",
    "\n",
    "if overwrite:\n",
    "    etl_pipeline_raw(\n",
    "        data_dir=RAW_LIB_DIR,\n",
    "        dirty_st_path=DIRTY_ST,\n",
    "        ct_pw=CT_PW,\n",
    "        ct_un=CT_UN,\n",
    "        con=con,\n",
    "        overwrite=True,\n",
    "        run_extraction=True,\n",
    "        excluded_samples=[\n",
    "            {\n",
    "                \"samplecode\": \"2021-debortoli-cabernet-merlot_avantor\",\n",
    "                \"reason\": \"aborted run\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    dset.to_netcdf(NC_RAW)\n",
    "else:\n",
    "    dset = xr.open_dataset(NC_RAW)\n",
    "\n",
    "dset = dset.assign_coords({\"wavelength\": dset[\"wavelength\"].astype(int)})\n",
    "dset.pipe(display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a demonstration.. the red shiraz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shiraz = dset.sel(color=\"red\", varietal=\"shiraz\", wavelength=256)\n",
    "# #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shiraz = dset.sel(id=[\"e56c4dcd-2847-4d34-b457-743be10b0608\"])\n",
    "# shiraz\n",
    "# con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually appears that you cant actually subset xarray Datasets..\n",
    "\n",
    "Considering that the data is already setup in the database, I think it would be better to go back to SQL first.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = (\n",
    "#     dset.img.sel(id=\"e56c4dcd-2847-4d34-b457-743be10b0608\").to_dataframe().reset_index()\n",
    "# )\n",
    "# id = df[\"id\"][0]\n",
    "# df_ = df.drop(\"id\", axis=1)\n",
    "# tidy_df = df_.set_index(\"mins\").pivot(columns=\"wavelength\", values=\"img\")\n",
    "\n",
    "# db.sql(\n",
    "#     \"\"\"--sql\n",
    "#     select \"190\" from tidy_df\n",
    "#     \"\"\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from database_etl.etl.sql.to_xr.sql_to_xr import get_imgs_as_dict\n",
    "\n",
    "# result = get_imgs_as_dict(con=con, m=7800)\n",
    "# list(result.values())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavelengths = list(result.values())[0].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy_imgs = (\n",
    "#     pl.from_pandas(img.assign(**{\"id\": id}).reset_index()) for id, img in result.items()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can insert, but then cant have primary keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dset\n",
    "# del df\n",
    "# del df_\n",
    "# del result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_t2(con, wavelengths):\n",
    "#     wavelength_col_decs = \",\".join(\n",
    "#         [f'\"{x}\" float' for x in wavelengths if x not in [\"mins\", \"id\"]]\n",
    "#     )\n",
    "\n",
    "#     con.sql(\n",
    "#         f\"\"\"--sql\n",
    "#     drop table t2;\n",
    "#     create or replace table t2 (\n",
    "#         id varchar references chm(id),\n",
    "#         mins float,\n",
    "#         {wavelength_col_decs},\n",
    "#         primary key (id, mins)\n",
    "#         );\n",
    "#     \"\"\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# def insert_img(con, img, wavelengths):\n",
    "#     for x in img.partition_by(\"mins\"):\n",
    "#         # display(x)\n",
    "#         con.sql(\n",
    "#             f\"\"\"--sql\n",
    "#         insert into t2\n",
    "#             select\n",
    "#                 id,\n",
    "#                 mins,\n",
    "#                 {\",\".join([f'\"{x}\"' for x in wavelengths])}\n",
    "#             from\n",
    "#                 x\n",
    "#         \"\"\"\n",
    "#         )\n",
    "\n",
    "\n",
    "# def insert_imgs(con, imgs, wavelengths):\n",
    "#     for idx, img in enumerate(imgs):\n",
    "#         print(idx, img[\"id\"][0])\n",
    "#         insert_img(con=con, img=img, wavelengths=wavelengths)\n",
    "\n",
    "\n",
    "# def create_tidy_img_tbl(con, imgs, wavelengths) -> None:\n",
    "#     create_tidy_img_tbl(con=con, imgs=imgs, wavelengths=wavelengths)\n",
    "#     insert_imgs(con, imgs, wavelengths=wavelengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavelengths = [x for x in wavelengths if x not in [\"id\", \"mins\"]]\n",
    "# shortened_wavelengths = wavelengths[:5]\n",
    "# print(shortened_wavelengths)\n",
    "# create_t2(con=con, wavelengths=shortened_wavelengths)\n",
    "# insert_imgs(con, tidy_imgs, wavelengths=shortened_wavelengths)\n",
    "\n",
    "# con.sql(\n",
    "#     \"\"\"--sql\n",
    "# select\n",
    "#     *\n",
    "# from\n",
    "#     t2\n",
    "# limit 10\n",
    "# \"\"\"\n",
    "# ).pl()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as promising as this is, its obviosuly making something upset. It's just not worth the effort to make this work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "While it did look possible to create a tidy image table, insertion of data proved impossible, resulting in the kernel dying more often than not. Evidently the mechanism wasnt intended for this volume of insertion.\n",
    "\n",
    "We could try reading the parquet files directly, or inserting one row at a time..\n",
    "\n",
    "Ok look we could play around with it like this all week, but its evident that duckdb doesnt like my data. End of the day, we're not looking to query the raw data anyway, and furthermore 99% of the wavelengths dont contain any useful information anyway.\n",
    "\n",
    "Create a rough module to translate the results of a query to filepaths then return the selected data as a list/generator. Load it into a tensor and fire away. Anything else is a WASTE OF TIME. Or query on the dset, selecting by ID.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca-analysis-6KQS4gUX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
