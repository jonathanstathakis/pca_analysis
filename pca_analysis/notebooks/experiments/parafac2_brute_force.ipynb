{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Brute Force PARAFAC2\n",
    "cdt: 2024-12-03T12:15:03\n",
    "description: \"An experiment on brute forcing a solution to the decomposition of the shiraz dataset through 'manual' iteration until an acceptable model is formed\"\n",
    "status: open\n",
    "conclusion: \"\"\n",
    "project: parafac2\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Froce PARAFAC2 Decomposition of Shiraz Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is a reconstruction of an experiment which was accidentally deleted.\n",
    "\n",
    "Efforts of using CORCONDIA to estimate the rank of the dataset have failed. Thus we will attempt to 'brute force' a solution. This will provide insight into how the model changes over increasing rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3 --print\n",
    "\n",
    "# get the test data as two tables: metadata and a samplewise stacked img table\n",
    "import logging\n",
    "\n",
    "import duckdb as db\n",
    "import plotly.express as px\n",
    "import polars as pl\n",
    "from database_etl import get_data\n",
    "from pca_analysis import xr_signal\n",
    "\n",
    "from pca_analysis.definitions import DB_PATH_UV\n",
    "from pca_analysis.get_sample_data import get_ids_by_varietal\n",
    "import plotly.io as pio\n",
    "import xarray as xr\n",
    "import darkdetect\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "xr.set_options(display_expand_data=False, display_expand_coords=False)\n",
    "\n",
    "if darkdetect.isDark():\n",
    "    pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "\n",
    "with db.connect(DB_PATH_UV) as conn:\n",
    "    ids = get_ids_by_varietal(\"shiraz\", conn)\n",
    "\n",
    "    ds = get_data(output=\"xr\", con=conn, runids=ids)\n",
    "\n",
    "# replace id with id_rank to be more human friendly\n",
    "ds = (\n",
    "    ds.assign_coords(\n",
    "        id_rank=lambda x: (\n",
    "            \"id\",\n",
    "            x.coords[\"id\"].to_dataframe()[\"id\"].rank(method=\"dense\").astype(int),\n",
    "        )\n",
    "    )\n",
    "    .sel(wavelength=\"256\")\n",
    "    .swap_dims({\"id\": \"id_rank\"})\n",
    ")\n",
    "ds = ds.rename({\"imgs\": \"raw_data\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.pipe(\n",
    "    xr_signal.facet_plot_multiple_traces,\n",
    "    grouper=[\"id_rank\"],\n",
    "    data_keys=[\"raw_data\"],\n",
    "    x=\"mins\",\n",
    "    trace_kwargs=[dict(mode=\"lines\", line=dict(color=\"cadetblue\"))],\n",
    "    col_wrap=3,\n",
    "    fig_kwargs=dict(y_title=\"au\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, sample 2 is very much an outlier when compared to the other samples, and will be removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.where(lambda x: x.id_rank != 2, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning (Peak Picking)\n",
    "\n",
    "- @ianeselli_completeanalysispipeline_2024 used HAC. They preprocessed with smoothing and peak alignment and used unsupervised HAC with Euclidean distance metric and linkage and dendrogram height equal to the average width of the peaks.\n",
    "- @sinanian_multivariatecurveresolutionalternating_2016 binned to unit masses, but also discussed @bedia_compressionstrategieschemometric_2016 defining Regions of Interest (ROI), stating that while it is useful for greatly eliminating unnecessary data, it can miss low intensity peaks if the threshold is set too high.\n",
    "- @bedia_compressionstrategieschemometric_2016 describes a feature detection and data compression method based on the *centWave* algorithm.\n",
    "- @anthony_libraryintegratedsimplismaalsdeconvolution_2022 manually binned the peaks.\n",
    "- @anthony_libraryintegratedsimplismaalsdeconvolution_2022 state that simple models are better at peak picking than complicated, abstract ones. They describe current issues with peak picking as a significant bottleneck in metabolomic studies.\n",
    "- @haas_opensourcechromatographicdata_2023 released a Python GUI package for analysis of HPLC-DAD data including peak picking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Subtraction\n",
    "\n",
    "To simplify tool development, we should first subtract the baseline from each sample. Whether or not there is a baseline is questionable, however the rise and fall does roughly correspond with the change in concentration of methanol in the mobile phase, potentially introducing background absorption. Either way, the data will be easier to work with with zeroed baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pybaselines.smooth import snip\n",
    "\n",
    "\n",
    "def apply_snip(da: xr.DataArray, **kwargs):\n",
    "    \"\"\"\n",
    "    docs: https://pybaselines.readthedocs.io/en/latest/api/pybaselines/smooth/index.html\n",
    "    \"\"\"\n",
    "    blines = []\n",
    "    for x in da:\n",
    "        bline, _ = snip(x, **kwargs)\n",
    "        blines.append(bline)\n",
    "\n",
    "    blines_ = np.stack(blines)\n",
    "    blines_da = da.copy(data=blines_)\n",
    "    blines_da.name = \"baselines\"\n",
    "\n",
    "    da_ = xr.merge([da, blines_da])\n",
    "    da_ = da_.assign(data_corr=lambda x: x[\"raw_data\"] - x[\"baselines\"])\n",
    "    return da_\n",
    "\n",
    "\n",
    "ds = ds.raw_data.pipe(apply_snip, max_half_window=30).where(\n",
    "    lambda x: x.mins < 30, drop=True\n",
    ")\n",
    "(\n",
    "    ds.pipe(\n",
    "        xr_signal.facet_plot_multiple_traces,\n",
    "        grouper=[\"id_rank\"],\n",
    "        data_keys=[\"raw_data\", \"data_corr\"],\n",
    "        x=\"mins\",\n",
    "        trace_kwargs=[\n",
    "            dict(mode=\"lines\", line=dict(color=\"cadetblue\"), opacity=0.55),\n",
    "            dict(mode=\"lines\", line=dict(color=\"red\"), opacity=0.95),\n",
    "        ],\n",
    "        col_wrap=3,\n",
    "        fig_kwargs=dict(\n",
    "            y_title=\"au\",\n",
    "        ),\n",
    "    ).update_layout(height=1500)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to choose a test set. This is a region with a reasonably flat baseline on either side of a peak cluster, and a cluster of 3 or so peaks. One method of finding the points between clusters is to find local minima. The easiest way to do this is to invert the signal and run a peak finding routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "After gross baseline removal comes smoothing. The criteria is that with the default find_peaks params, no peaks are detected before the first 0.77 seconds. This can be achieved through savgol smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.preprocessing.pipeline import clustering_by_maxima\n",
    "\n",
    "ds_, box = clustering_by_maxima(\n",
    "    ds=ds,\n",
    "    signal_key=\"data_corr\",\n",
    "    x_key=\"mins\",\n",
    "    grouper=[\"id_rank\"],\n",
    "    by_maxima=True,\n",
    "    # savgol_kwargs=dict(\n",
    "    #     polyorder=2,\n",
    "    #     window_length=70,\n",
    "    # ),\n",
    "    display_facet_peaks_plot=True,\n",
    "    display_cluster_table=True,\n",
    "    facet_peak_plot_kwargs=dict(\n",
    "        col_wrap=3,\n",
    "    ),\n",
    "    find_peaks_kws=dict(rel_height=0.5, prominence=2),\n",
    "    clustering_kws=dict(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=2,\n",
    "        linkage=\"average\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# when not using `n_clusters`, lower `distance_threshold` increases number of\n",
    "# clusters.\n",
    "\n",
    "# when clustering by minima, if one signals minima coincides with another's maxima,\n",
    "# that peak will be cut. Thi     s is not desired.\n",
    "box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the results of which are quite acceptable. Without much fiddling, ranges are identified within which a reasonable amount of peaks fall (2 > x < 6). The only draw back is that some of the parameter values are currently hard coded data dependent values, meaning that a different baseline subtraction will require different values. A problem to be solved down track, but essentially means that every run will require a little manual tuning.\n",
    "\n",
    "So now we will take a moment to study clustering of 1D arrays. [clustering](../clustering.ipynb). \n",
    "\n",
    "@ianeselli_completeanalysispipeline_2024 used heirarchical agglomerative clustering to cluster the peak maxima with a Euclidean distance metric and used average width linkage distance threshold (average linkage).\n",
    "\n",
    "\n",
    "Observations:\n",
    "- clustering on the whole dataset doesnt make any sense. Clustering on the whole signal would simply cluster accordnig to peak heights, i.e. along the y-axis, rather than the x. Thats why we detect peaks first. Either the minima or maxima. Now, Ianeselli chose to cluster the peak maxima rather than minima..\n",
    "- maximising the number of peaks maximises the extent of the signal captured into all clusters. This is actually beneficial for rough binning.\n",
    "- more often than not, the center of the inter-cluster regions approximate a local minima, meaning that splitting the interpeak areas between the two clusters is a good method of ensuring that all peak width is captured by its cluster region.\n",
    "- the problem of cluster labelling is akin to a [gap and island problem](https://mattboegner.com/improve-your-sql-skills-master-the-gaps-islands-problem/)\n",
    "\n",
    "TODO add a padding parameter\n",
    "TODO add a smoothing and sharpening routine.\n",
    "TODO achieve average peak density of 3 peaks per cluster. With sufficient smoothing/sharpening this should be possible.\n",
    "TODO add demonstrations for other cluster methods (integrate into function?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca-analysis-6KQS4gUX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
