{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: PARAFAC 2 on Wine\n",
    "description: first proper attempt to decompose wine dataset.\n",
    "project: parafac2\n",
    "conclusion: \"\"\n",
    "status: open\n",
    "cdt: 2024-09-26T15:07:44\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "import duckdb as db\n",
    "from pca_analysis.get_sample_data import get_shiraz_data\n",
    "from pca_analysis.definitions import DB_PATH_UV\n",
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "pl.Config.set_tbl_rows(99)\n",
    "\n",
    "\n",
    "con = db.connect(DB_PATH_UV, read_only=True)\n",
    "\n",
    "shiraz_data = get_shiraz_data(con=con)\n",
    "\n",
    "shiraz_data[0][0].head()\n",
    "len(shiraz_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [tup[0][\"id\"][0] for tup in shiraz_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, sample 82 is questionably small in scale relative to the other samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make the tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select a subset to begin.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_tensor(\n",
    "#     data: list[tuple[pl.DataFrame, pl.DataFrame]],\n",
    "# ) -> tuple[list[str], NDArray]:\n",
    "#     \"\"\"\n",
    "#     todo: add test for numeric col only in x[0]\n",
    "#     \"\"\"\n",
    "#     t: list[pl.DataFrame] = [x[0].drop([\"mins\", \"path\", \"runid\", \"id\"]) for x in data]\n",
    "\n",
    "#     ids: list[str] = [x[0][\"runid\"][0] for x in data]\n",
    "\n",
    "#     tt = np.stack([x[1].to_numpy() for x in t])\n",
    "\n",
    "#     return ids, tt\n",
    "\n",
    "\n",
    "# tt = prepare_tensor(data=shiraz_data)\n",
    "# len(tt[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're expecting a tensor with dimensions m observations, n wavelengths and o samples. For the **shiraz** dataset, this is 7800 x 106 x 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomp = parafac2(\n",
    "#     tensor_slices=tt[1], rank=22, return_errors=True, n_iter_max=500, nn_modes=\"all\"\n",
    "# )\n",
    "# decomp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    imgs: pl.DataFrame = pl.concat([data[0] for data in data])\n",
    "    mta: pl.DataFrame = pl.concat([data[1] for data in data])\n",
    "\n",
    "    imgs.head()\n",
    "\n",
    "    mta = mta.join(imgs.select(\"runid\", \"path\").unique(), on=\"runid\", how=\"left\")\n",
    "    imgs = imgs.drop(\"path\", \"id\")\n",
    "\n",
    "    imgs_ = (\n",
    "        imgs.unpivot(\n",
    "            index=[\"runid\", \"mins\"],\n",
    "            variable_name=\"wavelength\",\n",
    "            value_name=\"abs\",\n",
    "        )\n",
    "        .select(\n",
    "            \"runid\",\n",
    "            pl.col(\"wavelength\").cast(int),\n",
    "            \"mins\",\n",
    "            \"abs\",\n",
    "        )\n",
    "        .sort(\"runid\", \"wavelength\", \"mins\")\n",
    "    )\n",
    "\n",
    "    return imgs_, mta\n",
    "\n",
    "\n",
    "imgs, mta = prepare_data(data=shiraz_data)\n",
    "mta.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the data at 256.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.filter((pl.col(\"wavelength\").eq(256)).or_(pl.col(\"wavelength\").eq(330))).plot.line(\n",
    "    x=\"mins\", y=\"abs\", color=\"wavelength:N\"\n",
    ").facet(\"runid\", columns=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. 82 doesnt look interesting, lets get rid of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = imgs.filter(pl.col(\"runid\").ne(\"82\"))\n",
    "imgs[\"runid\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slimming The Dataset\n",
    "\n",
    "There will obviosuly be a section of each image which contains more information by proportion than other sections.\n",
    "Thus as a first go we should try to reduce the time and wavelength modes as much as possible. The first can be done\n",
    "first by cutting baseline past the intersection with the origin, say ~30 mins from memory, and the second by first\n",
    "doing the same, dropping any points after a return to origin.\n",
    "\n",
    "After that if ewe decide that the tensors are still too large we can look at resampling by aggregation in the \n",
    "wavelength and time modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Snipping\n",
    "\n",
    "What is the time point at which we can comfortably cut the times? Where do the baselines return to zero across all wavelengths? Something like.. after smoothing, where is an inflection point? Do we need to subtract a baseline first?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.filter(\n",
    "    pl.col(\"wavelength\").is_in([256, 330]),\n",
    "    runid=\"84\",\n",
    ").with_columns(pl.col(\"wavelength\")).plot.line(\n",
    "    x=\"mins\", y=\"abs\", color=\"wavelength:N\"\n",
    ").properties(width=1000, height=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the two regions, ~256 and ~330 have their own distinctive features, indicating that there is value in preserving a wide range of wavelengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that there is a baseline. Lets remove it with asls. Infact, pybaselines comes with a native 2D implementation.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like its taking about a minute a run. Once we identify time/wavelengths to eliminate \n",
    "it will be faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pybaselines import Baseline2D\n",
    "\n",
    "# for run in imgs.partition_by(\"runid\")[0:1]:\n",
    "#     tidy: pl.DataFrame() = run.pivot(on=\"wavelength\", values=\"abs\")\n",
    "\n",
    "#     # print(tidy.schema)\n",
    "\n",
    "#     # display(tidy.head())\n",
    "#     fitter = Baseline2D()\n",
    "#     min_unid = tidy.select(\"runid\", \"mins\")\n",
    "#     img = tidy.drop([\"runid\", \"mins\"])\n",
    "#     np_img = img.to_numpy()\n",
    "#     img_schema = img.schema\n",
    "\n",
    "#     baseline, params = fitter.asls(tidy.drop([\"runid\", \"mins\"]).to_numpy(writable=True))\n",
    "\n",
    "#     baseline_df = pl.DataFrame(data=baseline, schema=img_schema)\n",
    "#     new_df = pl.DataFrame(\n",
    "#         pl.concat([min_unid, baseline_df], how=\"horizontal\"),\n",
    "#         schema=tidy.schema,\n",
    "#     )\n",
    "#     display(new_df.head())\n",
    "\n",
    "# # display(baseline_df.head())\n",
    "\n",
    "# display(\n",
    "#     new_df.unpivot(\n",
    "#         index=[\"runid\", \"mins\"], value_name=\"abs\", variable_name=\"wavelength\"\n",
    "#     )\n",
    "#     .with_columns(pl.col(\"wavelength\").cast(int))\n",
    "#     .filter(pl.col(\"wavelength\").is_in([256, 330]))\n",
    "#     .plot.line(x=\"mins\", y=\"abs\", color=\"wavelength:N\")\n",
    "#     .properties(width=1000)\n",
    "# )\n",
    "\n",
    "# new_df_l = new_df.unpivot(\n",
    "#     index=[\"runid\", \"mins\"], value_name=\"abs\", variable_name=\"wavelength\"\n",
    "# )\n",
    "\n",
    "# display(new_df_l.head())\n",
    "\n",
    "# to graphically compare the signal and fitted baseline, join the tables\n",
    "# joined = imgs.filter(runid=\"0101\").join(\n",
    "#     new_df_l.filter(runid=\"0101\").with_columns(pl.col(\"wavelength\").cast(int)),\n",
    "#     on=[\"runid\", \"wavelength\", \"mins\"],\n",
    "#     how=\"left\",\n",
    "#     suffix=\"_baseline\",\n",
    "# )\n",
    "\n",
    "# display(joined.head())\n",
    "\n",
    "# joined_l = (\n",
    "#     joined.rename({\"abs\": \"ing_sig\"})\n",
    "#     .unpivot(\n",
    "#         index=[\"runid\", \"wavelength\", \"mins\"], value_name=\"abs\", variable_name=\"signal\"\n",
    "#     )\n",
    "#     .sort(\"runid\", \"signal\", \"wavelength\", \"mins\")\n",
    "# )\n",
    "\n",
    "# display(\n",
    "#     joined_l.filter(pl.col(\"wavelength\").is_in([256, 330]))\n",
    "#     # .with_columns(pl.concat_str([\"wavelength\", \"signal\"]).alias(\"nm_sig\"))\n",
    "#     .plot.line(x=\"mins\", y=\"abs\", color=\"signal\")\n",
    "#     .facet(\"wavelength\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see, very poor baseline fit. we should consolidate the processes created thus far and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling\n",
    "\n",
    "To develop a pipeline we need execution to be quick, thus we'll downsample to a respectable coarseness, develop the pipeline, then observe the results as fineness is increased. One way of downsampling is to calculate rolling averages for a given window. In this case, it is logical to first downsample by time then by wavelength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To downsample by time we need to express time units as datetime, or integer. A natural method is to use a numeric index, running from 0 to n where n is the number of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = imgs.select(\n",
    "    pl.col(\"mins\").rank(method=\"dense\").over([\"runid\", \"wavelength\"]).alias(\"idx\"),\n",
    "    pl.exclude(\"idx\"),\n",
    ")\n",
    "imgs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(imgs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach taken from <https://stackoverflow.com/questions/70327284/how-can-we-resample-time-series-in-polars>\n",
    "\n",
    "\n",
    "def downsample_by_time(\n",
    "    df: pl.DataFrame,\n",
    "    index_col: str,\n",
    "    every: str,\n",
    "    group_by: str,\n",
    "    cols_to_perserve: list[str] = [],\n",
    ") -> pl.DataFrame:\n",
    "    # if cols_to_perserve:\n",
    "    #     pres_cols = [pl.col(col) for col in cols_to_perserve]\n",
    "\n",
    "    if not isinstance(cols_to_perserve, list):\n",
    "        raise TypeError(\"cols_to_preserve must be list of string\")\n",
    "\n",
    "    for col in cols_to_perserve:\n",
    "        if not isinstance(col, str):\n",
    "            raise TypeError(\"expect elements of cols_to_preserve to be str\")\n",
    "\n",
    "    df_ = (\n",
    "        df.with_columns(pl.col(index_col).cast(pl.Int64))\n",
    "        .group_by_dynamic(index_column=index_col, every=every, group_by=group_by)\n",
    "        .agg(pl.mean(\"abs\"), *[pl.first(col) for col in cols_to_perserve])\n",
    "    )\n",
    "\n",
    "    return df_\n",
    "    # .group_by(\"runid\", \"wavelength\").len()\n",
    "\n",
    "\n",
    "downsampled = downsample_by_time(\n",
    "    df=imgs,\n",
    "    index_col=\"idx\",\n",
    "    every=\"10i\",\n",
    "    group_by=[\"runid\", \"wavelength\"],\n",
    "    cols_to_perserve=[\"mins\"],\n",
    ")\n",
    "\n",
    "\n",
    "downsampled.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_10_line = downsampled.filter(runid=\"0101\", wavelength=256).select(\n",
    "    \"runid\", pl.lit(10).alias(\"downsampling_factor\"), \"idx\", \"mins\", \"abs\"\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    \"runid\",\n",
    "    pl.lit(0).alias(\"downsampling_factor\"),\n",
    "    \"idx\",\n",
    "    \"mins\",\n",
    "    \"abs\",\n",
    "]\n",
    "img = imgs.filter(runid=\"0101\", wavelength=256).select(cols)\n",
    "pl.concat([ds_10_line, img], how=\"vertical_relaxed\").sort(\n",
    "    [\"downsampling_factor\", \"mins\"]\n",
    ").plot.line(x=\"mins\", y=\"abs\", color=\"downsampling_factor:N\").properties(\n",
    "    width=1000, height=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the downsampling has a marked effect on the height of the most intense peaks, however the peaks are widened, and the area is unchanged, according to <https://terpconnect.umd.edu/~toh/spectrum/Integration.html#:~:text=Incidentally%2C%20smoothing%20a%20noisy%20signal,the%20overlap%20between%20adjacent%20peaks>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to divide the signal into regions for easier viewing. Bin 1 will run from 0 to 4 mins, bin 2 from 4 t0 8, bin 3 from 8 to 15, bin 4 from 15 to 19, bin 5 from 19 to 22.7, bin 6 from 22.7 to 27, bin 7 the remainder.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [4, 8, 15, 19, 22.7, 27]\n",
    "\n",
    "mins = img.select(\"mins\")\n",
    "time_labels = mins.with_columns(\n",
    "    pl.col(\"mins\").cut(bins).alias(\"bins\"), pl.col(\"mins\").rank(\"dense\").alias(\"idx\")\n",
    ")\n",
    "\n",
    "img.join(time_labels.drop(\"mins\"), on=\"idx\").plot.line(x=\"mins\", y=\"abs\").facet(\n",
    "    \"bins\",\n",
    "    columns=3,\n",
    ").resolve_scale(x=\"independent\", y=\"independent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, it appears that you can see a spike at ~42 mins which corresponds to the point where the mobile phase returns to a ratio of 95% water to 5% methanol and reequlibreation prior to the next run commences. So, it should be noted that any time after this should be disregarded. More to the point, we should note what each region corresponds to, according to the mobile phase. 0 to 38 mins sees a 2.5% per minute increase in the proportion of methanol until the mobile phase is 100% methanol at that point. It is then held for two minutes before returning to the initial ratio from 40 to 42. We can observe the character of the signal within these intervals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Gradient Table to Signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the regions of the signal, we can map the change in mobile phase composition over time to the signal, then divide it based on the mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the intervals are labelled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mob_phase_changes_to_time_labels(\n",
    "    df: pl.DataFrame, right_ends: list[float], labels: list[str]\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    define\n",
    "    \"\"\"\n",
    "    return df.with_columns(\n",
    "        pl.col(\"mins\").cut(breaks=right_ends, labels=labels).alias(\"intvls\")\n",
    "    )\n",
    "\n",
    "\n",
    "def fetch_solvent_timetable(con: db.DuckDBPyConnection) -> pl.DataFrame:\n",
    "    solvent_timetable = con.execute(\n",
    "        \"\"\"--sql\n",
    "    select * from solvprop_over_mins where runid = ? order by runid, mins, channel\n",
    "    \"\"\",\n",
    "        parameters=[img.get_column(\"runid\")[0]],\n",
    "    ).pl()\n",
    "    return solvent_timetable\n",
    "\n",
    "\n",
    "def pivot_solvcomp_timetable(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return df.pivot(on=\"channel\", values=\"percent\").with_columns(\n",
    "        pl.col(\"mins\").rank(\"dense\").alias(\"mb_int\")\n",
    "    )\n",
    "\n",
    "\n",
    "def check_min_max_of_intervals(df: pl.DataFrame, grouper) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.group_by(grouper)\n",
    "        .agg(\n",
    "            pl.min(\"mins\").alias(\"min\"),\n",
    "            pl.max(\"mins\").alias(\"max\"),\n",
    "        )\n",
    "        .sort([\"min\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def add_starting_bs(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    starting_bs = df.select(\n",
    "        \"intvls\",\n",
    "        pl.col(\"b\").shift(1).alias(\"starting_b\"),\n",
    "        pl.col(\"mins\").shift(1).alias(\"starting_mins\"),\n",
    "    )\n",
    "\n",
    "    df = df.rename({\"b\": \"ending_b\"})\n",
    "\n",
    "    df_ = df.join(starting_bs, on=\"intvls\", how=\"left\")\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "def calc_grads(st_piv: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        st_piv.sort(\"intvls\")\n",
    "        .pipe(add_starting_bs)\n",
    "        .with_columns(\n",
    "            (pl.col(\"mins\") - pl.col(\"starting_mins\")).alias(\"mins_diff\"),\n",
    "            (pl.col(\"ending_b\") - pl.col(\"starting_b\")).alias(\"b_diff\"),\n",
    "        )\n",
    "        .with_columns((pl.col(\"b_diff\") / pl.col(\"mins_diff\")).alias(\"diff\"))\n",
    "        .select(\"intvls\", \"mins\", \"diff\", \"starting_b\")\n",
    "    )\n",
    "\n",
    "\n",
    "def join_solvcomp_over_time_img(\n",
    "    solvcomp_over_time: pl.DataFrame, img: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    return img.join(solvcomp_over_time, on=\"mins\", how=\"left\")\n",
    "\n",
    "\n",
    "def join_grads(intvl_labelled_times: pl.DataFrame, grads: pl.DataFrame) -> pl.DataFrame:\n",
    "    return intvl_labelled_times.join(\n",
    "        grads.select(\"intvls\", \"diff\", \"starting_b\"),\n",
    "        on=[\n",
    "            \"intvls\",\n",
    "        ],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_rel_mins(df):\n",
    "    timestep = df[\"mins\"].diff().mean()\n",
    "    return df.with_columns(\n",
    "        pl.col(\"mins\").rank(\"dense\").over(\"intvls\").mul(timestep).alias(\"rel_mins\")\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_b(df):\n",
    "    \"\"\"\n",
    "    to calc with diff, need the gradient at every time point\n",
    "\n",
    "    i = (i-1)*2.5 + init.\n",
    "    \"\"\"\n",
    "\n",
    "    return df.with_columns(\n",
    "        (pl.col(\"rel_mins\").mul(\"diff\") + pl.col(\"starting_b\")).alias(\"calc_b\")\n",
    "    )\n",
    "\n",
    "\n",
    "def add_b_prop_over_gradients(\n",
    "    intvl_labelled_times: pl.DataFrame, with_grads: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    solvcomp_over_time = (\n",
    "        intvl_labelled_times\n",
    "        .pipe(join_grads, grads=with_grads)\n",
    "        .pipe(add_rel_mins)\n",
    "        .pipe(calc_b)\n",
    "    )  # fmt: skip\n",
    "\n",
    "    return solvcomp_over_time\n",
    "\n",
    "\n",
    "def plot_signal_facet_intvls(signal_with_solvent_props: pl.DataFrame) -> alt.FacetChart:\n",
    "    exp_df_schema = pl.Schema(\n",
    "        {\n",
    "            \"mins\": pl.Float64(),\n",
    "            \"abs\": pl.Float64(),\n",
    "            \"intvls\": pl.Categorical(ordering=\"physical\"),\n",
    "        }\n",
    "    )\n",
    "    assert (\n",
    "        exp_df_schema\n",
    "        == signal_with_solvent_props.select(\"mins\", \"abs\", \"intvls\").schema\n",
    "    )\n",
    "\n",
    "    return signal_with_solvent_props.plot.line(x=\"mins\", y=\"abs\").facet(\n",
    "        \"intvls\", columns=3\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_prop_b_over_intvls_time(solcvomp_over_time: pl.DataFrame) -> alt.Chart:\n",
    "    return (\n",
    "        solvcomp_over_time.plot.scatter(x=\"mins\", color=\"intvls\", y=\"calc_b\")\n",
    "        .properties(title=\"prop b over intvls\")\n",
    "        .properties(height=400, width=1000)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvent_timetable = fetch_solvent_timetable(con=con)\n",
    "\n",
    "st_piv = solvent_timetable.pipe(pivot_solvcomp_timetable)\n",
    "\n",
    "breaks = st_piv.get_column(\"mins\").to_list()\n",
    "\n",
    "intvl_labels = [\"start\", \"methanol rise\", \"plateau\", \"water rise\", \"re-equilib\", \"\"]\n",
    "\n",
    "intvl_labelled_times = time_labels[[\"mins\"]].pipe(\n",
    "    add_mob_phase_changes_to_time_labels, right_ends=breaks, labels=intvl_labels\n",
    ")\n",
    "\n",
    "st_piv = st_piv.pipe(\n",
    "    add_mob_phase_changes_to_time_labels, right_ends=breaks, labels=intvl_labels\n",
    ")\n",
    "\n",
    "with_grads = calc_grads(st_piv=st_piv)\n",
    "display(with_grads)\n",
    "\n",
    "solvcomp_over_time = add_b_prop_over_gradients(\n",
    "    intvl_labelled_times=intvl_labelled_times, with_grads=with_grads\n",
    ")\n",
    "\n",
    "display(solvcomp_over_time.pipe(plot_prop_b_over_intvls_time))\n",
    "\n",
    "signal_with_solvent_props = solvcomp_over_time.pipe(\n",
    "    join_solvcomp_over_time_img, img=img\n",
    ")\n",
    "\n",
    "\n",
    "signal_with_solvent_props.select(\"mins\", \"abs\", \"intvls\").pipe(plot_signal_facet_intvls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see, the sample is fully eluted well before the methanol plateau, returning to baseline ~ 35 mins and varying around it. Smoothing such that the baseline in the 35 - 38min region is 0 could be an approach.\n",
    "\n",
    "TODO:\n",
    "- [ ] clean up the code above to be able to input any sample and produce the region labels based on input runid - get the mins and other inputs from the database.\n",
    "- [ ] write a function to cut the signal at the 38min mark AND produce a display of the methanol + re-equib that i can observe should I want to for quality control\n",
    "- [ ] Smooth signal\n",
    "- [ ] sharpen signal\n",
    "- [ ] baseline subtraction\n",
    "- [ ] decomposition..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
