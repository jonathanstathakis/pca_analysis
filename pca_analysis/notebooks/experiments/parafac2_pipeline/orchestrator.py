import duckdb as db
from pathlib import Path
from pca_analysis.notebooks.experiments.parafac2_pipeline.parafac2results import (
    Parafac2Results,
)
from pca_analysis.notebooks.experiments.parafac2_pipeline.data import Data
from pca_analysis.notebooks.experiments.parafac2_pipeline.pipeline import (
    create_pipeline,
)
from pca_analysis.notebooks.experiments.parafac2_pipeline.pipeline_defs import DCols
from pca_analysis.notebooks.experiments.parafac2_pipeline.input_data import InputData
import logging

logger = logging.getLogger(__name__)


def get_data_col_input():
    """the kwargs for the Data initialization"""

    return dict(
        time_col=str(DCols.TIME),
        runid_col=str(DCols.RUNID),
        nm_col=str(DCols.NM),
        abs_col=str(DCols.ABS),
        scalar_cols=[str(DCols.PATH), str(DCols.ID)],
    )


class Orchestrator:
    def __init__(self):
        """PARAFAC2 decomposition pipeline control singleton.

        Instructions For Use
        --------------------

        1. Initialise Orchastrator object.
        2. call `self.load_data`, providing a data object and optional filter expression
        3. call `self.run_pipeline`, providing an estimated rank and optional filter
        expression and database connection object (if you want to store the results in a
        persistent database)

        To obtain and inspect the results, access the `results` attribute generated in 3.

        TODO
        ----
        - [ ] add preprocessing stages i.e. baseline subtraction
        """
        self.input_data = Data(**get_data_col_input())  # ignore
        self._logfile = Path(__file__).parent / "pipeline_log"

    def load_data(
        self, con: db.DuckDBPyConnection, runids: list[str], filter_expr=None
    ):
        """
        Load the specified samples from the database with a optional filter expression.

        Adds a `input_data` attribute to self.

        :param filter_expr: a valid filter expression for the output table.
        :type filter_expr: pl.Expr | None
        :param con: a connection object to the database containing the sample data.
        :type con: db.DuckDBPyConnection
        :param runids: the desired runids
        :type runids: list[str]
        """
        logger.info("loading data..")
        self._con_input = con
        input_data = InputData(con=self._con_input, ids=runids)

        self.input_data = self.input_data.load_data(input_data).filter_nm_tbl(
            expr=filter_expr
        )

    def run_pipeline(
        self, filter_expr=None, rank=9, output_con: db.DuckDBPyConnection = db.connect()
    ) -> Parafac2Results:
        """
        run the pipeline from end to end

        creates and executes the internal PARAFAC2 decomposition pipeline then generates
        the results, returning the added `results` attribute. Also logs the fitting report
        generated by the pipeline to `self._logfile`

        :param filter_expr: an optional polars filter expression. Used for subsetting
        the data, defaults to None
        :type filter_expr: pl.Expr, optional
        :param rank: the rank of the data, defaults to 9
        :type rank: int, optional
        :param output_con: the connection to the results output database. Provide a
        filepath if a persistent database is desired, defaults to db.connect()
        :type output_con: db.DuckDBPyConnection, optional
        :return: a wrapper around the results with accessors and viz.
        :rtype: Parafac2Results
        """
        logger.info("running pipeline..")

        self._con_output = output_con

        if filter_expr:
            self.input_data = self.input_data.filter_nm_tbl(filter_expr)

        self._pipeline = create_pipeline(rank=rank)

        X = self.input_data.to_X()

        # to capture print for logs. See <https://johnpaton.net/posts/redirect-logging/>
        import contextlib

        with open(self._logfile, "w") as h, contextlib.redirect_stdout(h):
            self._decomp = self._pipeline.fit_transform(X)

        self.results = Parafac2Results(con=self._con_output, decomp=self._decomp)

        # display last two lines of the fit report (PARAFAC2)
        with open(self._logfile, "r") as f:
            logger.info("\n".join(f.readlines()[-2:]))

        self.results.create_datamart(input_imgs=X)
        return self.results
