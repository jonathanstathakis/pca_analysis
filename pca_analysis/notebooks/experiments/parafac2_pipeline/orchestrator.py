import duckdb as db
from pathlib import Path
from .data import Data
from .pipeline import (
    create_pipeline,
)
from .pipeline_defs import DCols
from .input_data import InputDataGetter
import logging
from copy import deepcopy
from typing import Self
from sklearn.pipeline import Pipeline
from .results_db import ResultsDB
from sqlalchemy import Engine, create_engine

import polars as pl

logger = logging.getLogger(__name__)


def get_data_col_input():
    """the kwargs for the Data initialization"""

    return dict(
        time_col=str(DCols.TIME),
        runid_col=str(DCols.RUNID),
        nm_col=str(DCols.NM),
        abs_col=str(DCols.ABS),
        scalar_cols=[str(DCols.PATH), str(DCols.ID)],
    )


class Orchestrator:
    def __init__(self, exec_id: str):
        """PARAFAC2 decomposition pipeline control singleton.

        Instructions For Use
        --------------------

        1. Initialise Orchastrator object.
        2. call `self.load_data`, providing a data object and optional filter expression
        3. call `self.run_pipeline`, providing an estimated rank and optional filter
        expression and database connection object (if you want to store the results in a
        persistent database)

        To obtain and inspect the results, access the `results` attribute generated in 3.
        """
        self._exec_id = exec_id
        self.input_data: Data
        self._logfile = Path(__file__).parent / "pipeline_log"
        self._pipeline: Pipeline
        self.results = None
        self._con_input: db.DuckDBPyConnection
        self._con_output: db.DuckDBPyConnection
        self._runids: list[str]
        self._wavelength_labels: list[int]
        self._time_labels: pl.DataFrame

    def __repr__(self):
        repr_str = f"""
        Orchestrator
        ------------
        ------------

        Input Data
        ----------
        
        {str(self.input_data)}

        Results
        -------

        {str(self.results)}
        """

        return repr_str

    def load_data(
        self,
        input_db_path: str | Path,
        runids: list[str],
        # wavelength_labels: list[int],
        filter_expr=None,
    ):
        """
        Load the specified samples from the database with a optional filter expression.

        Adds a `input_data` attribute to self.

        :param filter_expr: a valid filter expression for the output table.
        :type filter_expr: pl.Expr | None
        :param con: a connection object to the database containing the sample data.
        :type con: db.DuckDBPyConnection
        :param runids: the desired runids
        :type runids: list[str]
        """
        logger.info("loading data..")

        orc_ = deepcopy(self)
        orc_._con_input = db.connect(input_db_path)

        raw_data_extractor = InputDataGetter(conn=orc_._con_input, ids=runids)

        orc_.input_data = Data(**get_data_col_input())  # ignore

        orc_.input_data = orc_.input_data.load_data(raw_data_extractor)

        if filter_expr:
            orc_.input_data = orc_.input_data.filter_nm_tbl(expr=filter_expr)

        return orc_

    def select_data(self, filter_expr):
        """direct subsetting of the input data range."""

        orc_ = deepcopy(self)

        orc_.input_data = orc_.input_data.filter_nm_tbl(expr=filter_expr)

        return orc_

    def get_pipeline(self) -> Self:
        _orc = self._copy_orc()
        _orc._pipeline = create_pipeline()
        return _orc

    def set_params(self, params: dict) -> Self:
        _orc = self._copy_orc()
        _orc._pipeline.set_params(**params)
        return _orc

    def run_pipeline(
        self,
        filter_expr=None,
    ) -> Self:
        """
        run the pipeline from end to end

        creates and executes the internal PARAFAC2 decomposition pipeline then generates
        the results, returning the added `results` attribute. Also logs the fitting report
        generated by the pipeline to `self._logfile`

        :param filter_expr: an optional polars filter expression. Used for subsetting
        the data, defaults to None
        :type filter_expr: pl.Expr, optional
        :param rank: the rank of the data, defaults to 9
        :type rank: int, optional
        :param output_con: the connection to the results output database. Provide a
        filepath if a persistent database is desired, defaults to db.connect()
        :type output_con: db.DuckDBPyConnection, optional
        :return: a wrapper around the results with accessors and viz.
        :rtype: Parafac2Results
        """
        logger.info("running pipeline..")

        _orc = self._copy_orc()

        if filter_expr:
            _orc.input_data = _orc.input_data.filter_nm_tbl(filter_expr)

        X = _orc.input_data.to_X()

        _orc._runids = X.runids.get_column("runid").to_list()
        _orc._wavelength_labels = X.wavelength_labels.get_column("wavelength").to_list()
        _orc._time_labels = X.time_labels

        # to capture print for logs. See <https://johnpaton.net/posts/redirect-logging/>
        import contextlib

        with open(_orc._logfile, "w") as h, contextlib.redirect_stdout(h):
            _orc._pipeline.fit_transform(X.data)

        # display last two lines of the fit report (PARAFAC2)
        with open(_orc._logfile, "r") as f:
            logger.info("\n".join(f.readlines()[-2:]))

        return _orc

    def load_results(
        self,
        output_db_path: str,
        # output_con: db.DuckDBPyConnection = db.connect(),
    ) -> ResultsDB:
        """load pipeline results into a database, returning a ResultsDB object that provides methods of viewing the results"""

        pipeline = self._pipeline

        url = f"duckdb:///{output_db_path}"

        results_db = ResultsDB(
            engine=create_engine(url),
        )

        results_db.load_new_results(
            self._exec_id,
            self._runids,
            steps="all",
            pipeline=pipeline,
            wavelength_labels=self._wavelength_labels,
        )

        return results_db

    def _copy_orc(self) -> Self:
        """need to manually delete and reinitialise duckdb conn objects as they cant be pickled"""

        conn = self._con_input
        del self._con_input
        _orc = deepcopy(self)
        _orc._con_input = conn
        self._con_input = conn

        return _orc
