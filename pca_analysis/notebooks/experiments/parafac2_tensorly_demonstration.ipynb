{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "---\n",
    "cdt: 2024-08-30T10:53:39\n",
    "title: Demonstration of PARAFAC2 Implementation by TensorLy\n",
    "description: recreation of the PARAFAC2 demonstration by TensorLy\n",
    "conclusion: ''\n",
    "status: open\n",
    "project: parafac2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "TODO:\n",
    "- [ ] Morph this into a description of the decomposition based on the GC-MS data rather than the recreation. Move that to another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "\n",
    "from tensorly import parafac2_tensor\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pymatreader import read_mat\n",
    "from tensorly.parafac2_tensor import apply_parafac2_projections\n",
    "\n",
    "path = \"/Users/jonathan/mres_thesis/pca_analysis/Wine_v7.mat\"\n",
    "\n",
    "# keys have to be in this order to parse the data correctly\n",
    "key_it = [\"Label_Wine_samples\", \"Label_Elution_time\", \"Label_Mass_channels\"]\n",
    "data = read_mat(filename=path, variable_names=[\"Data_GC\"] + key_it)\n",
    "data.keys()\n",
    "\n",
    "raw_data = xr.DataArray(\n",
    "    data[\"Data_GC\"],\n",
    "    coords=[data[k] for k in key_it],\n",
    "    dims=[\n",
    "        \"sample\",\n",
    "        \"time\",\n",
    "        \"mz\",\n",
    "    ],\n",
    ").transpose(\"time\", \"mz\", \"sample\")\n",
    "raw_data.coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting the indexes to center the peaks\n",
    "\n",
    "times = data[\"Label_Elution_time\"]\n",
    "\n",
    "time_start = 16.52\n",
    "time_end = 16.76\n",
    "\n",
    "idx_start = np.nonzero(np.isclose(times, time_start, atol=1e-2))[0][0] - 6\n",
    "idx_end = np.nonzero(np.isclose(times, time_end, atol=1e-2))[0][0]\n",
    "print(idx_end - idx_start)\n",
    "time_start = times[idx_start]\n",
    "time_end = times[idx_end]\n",
    "\n",
    "sliced_data = raw_data[idx_start:idx_end, :, :]\n",
    "sliced_data.coords\n",
    "sliced_data.isel(sample=0).plot.line(x=\"time\", add_legend=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## PARAFAC2\n",
    "\n",
    "Since we know from the previous study that the estimated rank of 3 is correct, we can proceed to factorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "The metrics used by @zhang_flexibleimplementationtrilinearity_2022 are: lack of fit, explained variance, correlation coefficients and orthogonality angles. They should be implemented AFTER I understand how to extract the information from the decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following is taken from <https://tensorly.org/stable/auto_examples/decomposition/plot_parafac2.html>\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorly.decomposition import parafac2\n",
    "\n",
    "best_err = np.inf\n",
    "decomposition = None\n",
    "true_rank = 3\n",
    "\n",
    "for run in range(1):\n",
    "    print(f\"Training model {run}...\")\n",
    "    trial_decomposition, trial_errs = parafac2(\n",
    "        sliced_data.to_numpy(),\n",
    "        true_rank,\n",
    "        return_errors=True,\n",
    "        tol=1e-8,\n",
    "        n_iter_max=500,\n",
    "        random_state=run,\n",
    "    )\n",
    "    print(f\"Number of iterations: {len(trial_errs)}\")\n",
    "    print(f\"Final error: {trial_errs[-1]}\")\n",
    "    if best_err > trial_errs[-1]:\n",
    "        best_err = trial_errs[-1]\n",
    "        err = trial_errs\n",
    "        decomposition = trial_decomposition\n",
    "    print(\"-------------------------------\")\n",
    "print(f\"Best model error: {best_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### The Decomposition\n",
    "\n",
    "TensorLy, referencing @kiers_parafac2parti_1999, describes the PARAFAC2 decomposition [in their docs](https://tensorly.org/stable/modules/generated/tensorly.decomposition.Parafac2.html#r08ccc3506ae1-1) as:\n",
    "\n",
    "1. A three-mode (way) tensor $\\mathcal X \\in \\mathbb R^{I \\times J \\times K}$ of form $A[B_i]C$. One frontal slice. The $ith$ frontal slice, $X_i$ can be expressed as the product of three factor matrices: $X_i=B_i \\space \\text{diag}(a_i)\\space C^\\top$, where: \n",
    "   \n",
    "    1. $\\text{diag}(a_i)$ is a diagonal factor matrix. The non-zero entries of this matrix equal the $ith$ row of a $I \\times R$ factor matrix^[1] $A$.\n",
    "   \n",
    "    of $I \\times R$ dimensions\n",
    "    5. $A$\n",
    "    2. $B_i$ is a $J_i \\times R$ factor matrix such that $B^\\top_{i_1}B_{i_1}$ is constant for all $i$\n",
    "    3. $C$ is a $K \\times R$ factor matrix.\n",
    "\n",
    "2. TensorLy reformulates the following $B_i=P_iB$\n",
    "    - where: \n",
    "        1. $P_i$ is a $J_i \\times R$ orthogonal matrix\n",
    "        2. $B$ is a $R \\times R$ matrix.\n",
    "  \n",
    "The output is a `Parafac2Tensor` object that possesses the following attributes:\n",
    "\n",
    "1. weights: a 1D array of shape = rank\n",
    "2. factors: a list of the factors of the CP decomposition. element 1 is $I \\times R$, element 2 is $R \\times R$, element 3 is $K \\times R$\n",
    "3. projection_matrices: a list of the projection matrices used to create the evolving factors.\n",
    "\n",
    "The [PARAFAC2 demonstration](https://tensorly.org/stable/auto_examples/decomposition/plot_parafac2.html) describes the output (`Parafac2Tensor`) as the decomposition, and as \"a wrapper for.. ..the weights, factor matrices and projection matrices\". They say the weights are similar as to the output of a CP decomposition (possible more information there) but that the factor matrices $A$, $B$ and $C$ are stored in a tuple. Except that they are not, they are stored as a list. A minor point. To explain from the POV of the object - $A$, $B$, and $C$ are stored in the `factors` attriute as numpy arrays in the given order:\n",
    "\n",
    "[^1]: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(decomposition.factors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "display([type(f) for f in decomposition.factors])\n",
    "display([f.shape for f in decomposition.factors])\n",
    "\n",
    "\n",
    "weights, factors, projections = decomposition\n",
    "\n",
    "display(f\"{len(weights)=}\")\n",
    "display(f\"{type(weights)=}\")\n",
    "\n",
    "display(f\"{len(factors)=}\")\n",
    "display(f\"{type(factors)=}\")\n",
    "\n",
    "display(f\"{len(projections)=}\")\n",
    "display(f\"{type(projections)=}\")\n",
    "display(f\"{projections[0].shape=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors[0].shape, factors[1].shape, factors[2].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Applying Projections to B\n",
    "\n",
    "They note that $B$ is not $B_i$, rather it is a \"blueprint\" for $B_i$ and needs to be mu;tiplied by $P_i$, the $ith$ projection matrix. They go on to say that a helper function `tensorly.parafac2.apply_projections` (docs incorrectly call this `apply_projection_matrices`, is available to \"extract the informative $B_i$ factor matrices\".\n",
    "\n",
    "From the [code](https://github.com/tensorly/tensorly/blob/de78d0dd6ed1a935cc87a6c8ff6c73740ee3fcd0/tensorly/parafac2_tensor.py#L221), this function unpacks the `parafac2_tensor` object then computes the \"evolving factor\" as a series(list) of the dot products of the $ith$ projection $I \\times R$ and the 2nd element of the factors series $B$ $R \\times R$, producing $I \\times R$ matrices. Afterwards it returns a tuple of the weights and a nested tuple of the first factor matrix, $I \\times R$, the evolving factor vector, and the third matrix, $K \\times R$. So what this does is transform the second factor matrix through the projections.\n",
    "\n",
    "TODO: what is P, what is B, what is Bi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_evolving_factor_ith_calculation(parafac2_tensor):\n",
    "    weights, factors, projections = parafac2_tensor\n",
    "\n",
    "    ith_projection = projections[0]\n",
    "    factor_1 = factors[1]\n",
    "    ith_evolving_factor = np.dot(ith_projection, factor_1)\n",
    "\n",
    "    display(f\"{ith_projection.shape=}\")\n",
    "    display(f\"{factor_1.shape=}\")\n",
    "    display(f\"{ith_evolving_factor.shape=}\")\n",
    "\n",
    "\n",
    "demonstrate_evolving_factor_ith_calculation(decomposition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, factors = apply_parafac2_projections(decomposition)\n",
    "\n",
    "factors_0, evolving_factor, factors_1 = factors\n",
    "\n",
    "display(f\"factors_0:  {type(factors_0)},  {factors_0.shape}\")\n",
    "display(f\"evolving_factor:  {type(evolving_factor)},  {len(evolving_factor)}\")\n",
    "display(f\"factors_0:  {type(factors_1)},  {factors_1.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "It appears that $B$ consists of the spectral profile of each component. $C$ is the sample profile of each component, and $A$ is the time profile of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "They describe the factor matrices list consisting of elements representing each 'frontal slice', ~however I cant ratify that with what appears to be a length equal to the rank~ this may be because of the ordering of the modes. I have the ordered the modes as per @zhang_flexibleimplementationtrilinearity_2022 as elution time ('time'), mass channel ('mz'), and sample (\"sample\"), however, judging from the [tl-viz PARAFAC example](https://tensorly.org/viz/stable/auto_examples/plot_labelled_decompositions.html#sphx-glr-auto-examples-plot-labelled-decompositions-py) the ordering standard is sample first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Tensor Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "They state that reconstruction can be achieved through `tensorly.parafac2_tensor.parafac2_to_tensor` as long as the frontal slices are of even length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon = parafac2_tensor.parafac2_to_tensor(decomposition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(type(recon))\n",
    "display(recon.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recon[:, 39, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Checks out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Also we can access the frontal slices directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parafac2_tensor.parafac2_to_slices(decomposition))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "So in this case the frontal slices are the times. WHich tells me that Zhang et al. and TensorLy use different definitions. This can probably be ratified by observing the PARAFAC demonstration with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "They then go on to calculate the tucker congruence coefficient.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
