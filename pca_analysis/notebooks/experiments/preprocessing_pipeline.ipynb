{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Preprocessing\n",
    "cdt: 2024-12-03T12:15:03\n",
    "description: \"preprocessing. This can be the preprocessing demonstration and serve as an automated test until a more formal module can be established. It will need to individually demonstrate smoothing, sharpening and baseline subtraction.\"\n",
    "status: open\n",
    "conclusion: \"\"\n",
    "project: parafac2\n",
    "---\n",
    "\n",
    "TODO complete this notebook/module\n",
    "TODO select a smaller testset to speed up dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3 --print\n",
    "\n",
    "import logging\n",
    "from pca_analysis import xr_signal\n",
    "\n",
    "from pca_analysis.definitions import PARAFAC2_TESTSET\n",
    "from pca_analysis import xr_plotly\n",
    "import plotly.io as pio\n",
    "import xarray as xr\n",
    "import darkdetect\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "xr.set_options(display_expand_data=False, display_expand_coords=False)\n",
    "\n",
    "if darkdetect.isDark():\n",
    "    pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "ds = xr.load_dataset(PARAFAC2_TESTSET)\n",
    "\n",
    "# speed up development by using a subset.\n",
    "ds = ds.sel(wavelength=slice(210, 260, 5), mins=slice(0, 30))\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Subtraction\n",
    "\n",
    "To simplify tool development, we should first subtract the baseline from each sample. Whether or not there is a baseline is questionable, however the rise and fall does roughly correspond with the change in concentration of methanol in the mobile phase, potentially introducing background absorption. Either way, the data will be easier to work with with zeroed baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.preprocessing import bcorr\n",
    "\n",
    "\n",
    "ds = ds.pipe(bcorr.snip, core_dim=\"mins\", max_half_window=30)\n",
    "display(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_fig = (\n",
    "    ds.transpose(\"sample\", \"wavelength\", \"mins\")\n",
    "    .isel(wavelength=0)\n",
    "    .plotly.facet_plot_overlay(\n",
    "        grouper=\"sample\",\n",
    "        var_keys=[\"raw_data\", \"baselines\", \"data_corr\"],\n",
    "        col_wrap=3,\n",
    "        x_key=\"mins\",\n",
    "    )\n",
    ")\n",
    "overlay_fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "The criteria is that with the default find_peaks params, no peaks are detected before the first 0.77 seconds. This can be achieved through savgol smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.preprocessing import smooth\n",
    "\n",
    "(\n",
    "    ds.isel(sample=slice(2, 6))\n",
    "    .assign(\n",
    "        smoothed=ds.raw_data.pipe(\n",
    "            smooth.savgol_smooth,\n",
    "            input_core_dims=[\n",
    "                [\"mins\"],\n",
    "            ],\n",
    "            output_core_dims=[[\"mins\"]],\n",
    "            window_length=60,\n",
    "            polyorder=2,\n",
    "        )\n",
    "    )\n",
    "    .sel(wavelength=260, mins=slice(0, 10))\n",
    "    .plotly.facet_plot_overlay(\n",
    "        grouper=\"sample\", var_keys=[\"raw_data\", \"smoothed\"], col_wrap=2\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharpening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharpening is difficult to implement because there are no formal packages. This is because the algorithms are trivial.\n",
    "\n",
    "- https://terpconnect.umd.edu/~toh/spectrum/ResolutionEnhancement.html\n",
    "- @wahab_2019\n",
    "- https://dsp.stackexchange.com/questions/71297/why-is-peak-detection-in-chromatography-not-completely-automatic\n",
    "- https://bohr.wlu.ca/hfan/cp467/12/notes/cp467_12_lecture6_sharpening.pdf\n",
    "- Sharpening, like smoothing, is achieved via a filter.\n",
    "- unsharp masking subtracts the multiple of the laplacian from the signal multiplied by a factor $signal - a * laplacian(signal)$ https://www.idtools.com.au/unsharp-masking-with-python-and-opencv/\n",
    "- another approach to unsharp masking is to use a Gaussian filter https://stackoverflow.com/questions/4993082/how-can-i-sharpen-an-image-in-opencv\n",
    "- https://dsp.stackexchange.com/questions/70955/is-unsharp-mask-usm-equivalent-to-applying-laplacian-of-gaussian-filter-direct\n",
    "- another definition is given as: enchanced_image = original + amount * (original - blurred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsharp Masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.preprocessing import unsharp_mask\n",
    "\n",
    "# sharpen via laplacian\n",
    "(\n",
    "    ds.isel(sample=slice(2, 6))\n",
    "    .preproc.unsharp.laplacian(a=0.1, core_dims=[\"mins\"], var=\"raw_data\")\n",
    "    .isel(wavelength=3)\n",
    "    # .sel(mins=slice(0, 10))\n",
    "    .plotly.facet_plot_overlay(\n",
    "        grouper=\"sample\",\n",
    "        var_keys=[\"raw_data\", \"sharpened\", \"laplace\"],\n",
    "        col_wrap=2,\n",
    "        x_key=\"mins\",\n",
    "        trace_kwargs=dict(laplace=dict(opacity=0.3, line=dict(dash=\"dot\"))),\n",
    "    )\n",
    "    .update_layout(height=1000, title=dict(text=\"Sharpening via the Laplacian\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, while powerful, it has the effect of introducing negatives into the signal, which is something we definitely do not want. This is unavoidable as the negatives occur when a signal rapidly changes from baseline to peak, which is by definition a perfect chromatographic signal. Fiddling with the factor $a$ can result in an acceptable filter, however overall it is not ideal as we cannot avoid the negative. Good for a first pass sharpening though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsharp Masking with Gaussian Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater the sigma the less the filter fits the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis import preprocessing_xr\n",
    "\n",
    "(\n",
    "    ds.isel(\n",
    "        sample=slice(2, 6),\n",
    "    )\n",
    "    .preproc.unsharp.gaussian(var=\"raw_data\", core_dims=[\"mins\"], a=0.1, sigma=10)\n",
    "    .isel(wavelength=3)\n",
    "    .sel(mins=slice(0, 5))\n",
    "    .plotly.facet_plot_overlay(\n",
    "        grouper=\"sample\",\n",
    "        var_keys=[\"raw_data\", \"sharpened\", \"gaussian\"],\n",
    "        col_wrap=2,\n",
    "        x_key=\"mins\",\n",
    "        trace_kwargs=dict(gaussian=dict(opacity=0.3, line=dict(dash=\"dot\"))),\n",
    "    )\n",
    "    .update_layout(height=1000, title=dict(text=\"Sharpening via Gaussian Filter\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsharp masking by Gaussian filter requires to parameters - $\\sigma$ to define the filter and $a$, the strength of the mask. Fine tuning the parameters produces a result that is more gentle than the laplacian version but producs some odd mutations such as peaks becoming shorter while becoming sharper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpening After Baseline Subtraction\n",
    "\n",
    "As we can see in the previous example, the presence of a non-zero baseline makes sharpening difficult. Let's see the effect of sharpening after gross baseline removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "a = 0.05\n",
    "(\n",
    "    ds.isel(\n",
    "        sample=slice(2, 6),\n",
    "    )\n",
    "    .pipe(bcorr.snip, core_dim=\"mins\", max_half_window=30)\n",
    "    .assign(\n",
    "        laplace=lambda x: xr.apply_ufunc(\n",
    "            ndimage.laplace,\n",
    "            x[\"data_corr\"],\n",
    "            input_core_dims=[\n",
    "                [\"mins\"],\n",
    "            ],\n",
    "            output_core_dims=[\n",
    "                [\"mins\"],\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "    .assign(sharpened=lambda x: x[\"data_corr\"] + a * (x[\"raw_data\"] - x[\"laplace\"]))\n",
    "    .isel(wavelength=3)\n",
    "    # .sel(mins=slice(0, 10))\n",
    "    .plotly.facet_plot_overlay(\n",
    "        grouper=\"sample\",\n",
    "        var_keys=[\"data_corr\", \"sharpened\", \"laplace\"],\n",
    "        col_wrap=2,\n",
    "        x_key=\"mins\",\n",
    "        trace_kwargs=dict(laplace=dict(opacity=0.3, line=dict(dash=\"dot\"))),\n",
    "    )\n",
    "    .update_layout(height=1000)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the results are not significantly different. Interestingly the sharpening appears to add an artificual baseline in the mid of the signal. My conclusion is that sharpening is useful but not a conclusive or automated solution. I can forsee a future wherein a number of alternating sharpening and baseline subtraction steps are taken."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca-analysis-6KQS4gUX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
