{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Rank Estimation of GC-MS Data\n",
    "description: A test of rank estimation methods on the Zhang et al. GC-MS data to recreate their results\n",
    "project: parafac2\n",
    "conclusion: was able to reproduce their results for the specified peaks, but the inclusion of more peaks resulted in failure\n",
    "status: closed\n",
    "cdt: 2024-08-30T00:00:00\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Replication of @zhang_flexibleimplementationtrilinearity_2022 Rank Estimation\n",
    "\n",
    "This study replicated the reported results of @zhang_flexibleimplementationtrilinearity_2022 on a Wine GC-MS dataset. The dataset is described in section 3.2 and the results in 4.2.\n",
    "\n",
    "We first recreated the dataset and developed methods to preprocess, unfold, decompose and display the estimated singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The GC-MS data was downloaded from [here](https://ucphchemometrics.com/2023/06/01/wine-samples-analyzed-by-gc-ms-and-ft-ir-instruments/) and originally prepared by @skov_multiblockvariancepartitioning_2008. It is stored in MATLAB 7.3 format and required the use of [mat73](https://gitlab.com/obob/pymatreader/) library. Within the library is GC-MS, FT-I and physicochemical univariate measurements. The GC-MS data consists of 44 samples x 2700 elution time-points and 200 mass channels.\n",
    "\n",
    "The authors narrowed the scope to range from 16.52 to 16.76 mins (corresponding to a range of 25 units) containing two compounds (peaks), described in detail in section 3.2. They identified three significant components (chemical rank) attributing two to the compounds stated and one to the background. We expect to find the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pca_analysis.cabernet.shiraz.shiraz import Shiraz\n",
    "from pca_analysis.decomposers import PCA\n",
    "from pca_analysis.definitions import DATA_DIR\n",
    "from pca_analysis import xr_plotly\n",
    "from pca_analysis.cabernet.cabernet import Cabernet\n",
    "from pca_analysis import cabernet\n",
    "from pca_analysis.get_dataset import load_zhang_data\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "xr.set_options(display_expand_data=False)\n",
    "\n",
    "full_data = load_zhang_data()\n",
    "cabernet.chrom_dims.TIME = \"time\"\n",
    "cabernet.chrom_dims.SPECTRA = \"mz\"\n",
    "cabernet.chrom_dims.SAMPLE = \"sample\"\n",
    "\n",
    "cab = Cabernet(da=full_data)\n",
    "shz = cab[\"zhang_data\"]\n",
    "\n",
    "assert isinstance(shz, Shiraz)\n",
    "\n",
    "demo = shz.sel(time=slice(16.52 - 0.08, 16.76 + 0.08))\n",
    "\n",
    "display(demo)\n",
    "display(demo.sel(mz=slice(0, 100)).isel(sample=slice(0, 44, 4)).viz.heatmap(n_cols=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The data interval is displayed above and corresponds to that shown by @zhang_flexibleimplementationtrilinearity_2022 figure 5. The 2 peaks are best viewed at mz = 44, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.sel(mz=44).viz.line(x=\"time\", overlay_dim=\"sample\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Where we can see that the peaks are present in all samples at varying intensities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Unfolding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "@zhang_flexibleimplementationtrilinearity_2022 in section 2.5 describe the application of SVD (PCA) to different unfoldings of the data tensor. They state that depending on which mode is unfolded, the SVD will produce different numbers of significant components if the data is not perfectly trilinear, otherwise all three unfoldings will have the same number of significant components. They state that $X_{\\text{caug}}$ produces the most accurate estimate of significant components in the face of noise and trilinear disruption, assuming that each chemical species has a unique spectrum and their relative concentrations are independent. What is $X_{\\text{caug}}$? It is the unfolding $(I \\times K, J)$, which in the context of the dataset is $(\\text{retention times} \\times \\text{mz}, \\text{samples})$. Thus we first need to produce the augmented (unfolded) matrix $C_\\text{aug}$, unfolding along the sample mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.decomp.pca().scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Estimating the number of Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "While @zhang_flexibleimplementationtrilinearity_2022 specify the use of the SVD, a useful interface for estimating the number of components is sklearn's `PCA`. A rudimentary scree plot can be used to observe the inflection point described in figure 3. The authors state in section 4.1 that when observing a function of the explained variance against the number of components, the point where the explained variance does not \"change much anymore\" is the point where the components start describing the noise of the dataset rather than chemical species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "And as we can see, we're able to recreate the results if the cutoff of the magnitude of change is set to 0.005."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Full Dataset\n",
    "\n",
    "Now we'll see what happens if the same method is applied to the full dataset, as its only valid if it works for any number of peaks. If not then some underlying mechanism is at work. To reiterate, we're expecting the number of significant components == the number of peaks == the number of unique chemical species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Visual Estimation of Number of Peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "What are the number of components expected? It should be close to the number of peaks in the maximum mass channel. This is because its a fair assumption that the sample with the most abundant chemical species is also the most intense. To find this value we will find the mass channel with the highest amplitude then the sample with the highest average amplitude at that mass channel. That sample and mz is displayed below, with its peaks highlighted in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_spectra = shz.stats.mean_max_spectral_label\n",
    "max_sample_label = shz.stats.max_sample_label\n",
    "max_sample = shz.sel(sample=max_sample_label, mz=max_spectra)\n",
    "max_sample.sel(time=slice(4, 24)).viz.line(x=\"time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cab1 = cab.copy()\n",
    "max_sample = cab.sel(\n",
    "    sample=cab[\"zhang_data\"].stats.max_sample_label,\n",
    "    mz=cab[\"zhang_data\"].stats.mean_max_spectral_label,\n",
    ").pick_peaks(\"zhang_data\", find_peaks_kwargs=dict(height=0.005))\n",
    "\n",
    "display(max_sample.peak_array_as_df(\"peaks\"))\n",
    "display(max_sample.viz.overlay_peaks(\"zhang_data\", \"peaks\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_peaks = max_sample.peak_array_as_df(\"peaks\").shape[0]\n",
    "\n",
    "display(f\"{n_peaks=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(\n",
    "    f\"Visually it appears that all peaks are accounted for, thus the number of peaks is around {n_peaks}. Thus we should expe1ct around {n_peaks} significant components through PCA.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Unfold Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_data = shz\n",
    "zhang_data.decomp.pca().scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "But as we can see, the profile is coincidentally similar to the 2 peak slice with the vast majority of the variance explained by the first three compoonents, We can presume that this is because of a lack of scaling and centering distorting the model towards the largest features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Looks good. Whats the PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Scaled and Centered PCA Global Maxima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Scaling and centering can help models such as PCA fit noisy or otherwise abbhorant data. In this context, where after unfolding the dataset is represented by samples (sample and timewise rows) and features (spectral dimension columns), then centering is the subtraction of each columns mean from each row and scaling is the division of each by the columns standard variation. This is implemented by `sklearn`'s `StandardScaler`. The result is that each column ranges from 0 to 1 and has a mean of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "shz.decomp.pca(standardscale=True).scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "A reversal in results. It appears that PCA with this user defined metric estimates an optimal number of components equal to the number of peaks detected earlier. Evidently standard scaling the data meaningfully increases the number of significant components estimated by the scree plot. This indicates that it is an appropriate method of estimation for unaligned GC-MS data. To further explore the veracity of the result we would need to estimate the total number of unique peaks across the sample dimension, as this verification has only observed the sample with the absorbance maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## CORCONDIA\n",
    "\n",
    "CORCONDIA is a method of approximating the number of components of PARAFAC and PARAFAC-like models (i.e. PARAFAC2) through observation of a Tucker3 core [@bro_newefficientmethod_2003]. The algorithm iterates through components, starting at 1, until a user-specified limit, and we are looking for a steep dropoff as the indication of the optimal number of components. In the interest of speed we will restrict the signal to the 15 to 25 minute interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.sel(mz=44).isel(sample=slice(0, 44, 11)).viz.line(x=\"time\", overlay_dim=\"sample\")\n",
    "\n",
    "# display(zhang_demo_intvl)\n",
    "# display(\n",
    "#     zhang_demo_intvl.sel(\n",
    "#         mz=44,\n",
    "#     )\n",
    "#     .isel(sample=slice(0, 44, 11))\n",
    "#     .plotly.line(x=\"time\", color=\"sample\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "corcondia_results = demo.to_cabernet().rank_estimation.corcondia(\n",
    "    \"zhang_data\", rank_range=(1, 10)\n",
    ")\n",
    "display(corcondia_results.diagnostic_over_rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "As we can see, the CORCONDIA results are ambiguous over a wide range but indicate that a rank of 3 maintains model stability, which is in agreement with the PCA results. Note that while we havent' demonstrated it here ,the results are semi-random and that deserves more study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## PARAFAC2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### PARAFAC2 Demo Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "A demonstration of PARAFAC2 on the demo dataset. We demonstrated earlier that a rank of 3 is appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_range = 3\n",
    "\n",
    "demo = demo.to_cabernet().decomp.parafac2(\n",
    "    path=\"zhang_data\", rank=rank_range, n_iter_max=500, nn_modes=\"all\", linesearch=False\n",
    ")\n",
    "parafac2 = demo[\"parafac2\"]\n",
    "parafac2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = parafac2[\"components\"]\n",
    "components.sel(mz=44).viz.line(\n",
    "    x=\"time\",\n",
    "    facet_dim=\"sample\",\n",
    "    overlay_dim=\"component\",\n",
    "    n_cols=3,\n",
    ").update_layout(\n",
    "    title=\"samples per component of the Zhang et. al demo interval\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "This result corresponds to the deconstruction shown in @zhang_flexibleimplementationtrilinearity_2022, Figure 6.\n",
    "\n",
    "TODO make viz better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### PARAFAC2 Full Dataset\n",
    "\n",
    "We demonstrated earlier that through the PCA approach, we estimate 22 components for the full dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "The decomposition of the full dataset with rank = 22 and n_iter_max = 500 takes 7m 25s which is too slow to retain for testing purposes. Furthermore the results are disappointing, with some components capturing multiple peaks while others capture nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "parafac2_results_path = DATA_DIR / \"zhang_full_data_parafac2_decomp_results_v2.nc\"\n",
    "\n",
    "if not parafac2_results_path.exists():\n",
    "    cab = cab.decomp.parafac2(\n",
    "        path=\"zhang_data\", rank=22, verbose=True, n_iter_max=1, nn_modes=\"all\"\n",
    "    )\n",
    "    cab.to_netcdf(filepath=parafac2_results_path)\n",
    "else:\n",
    "    cab = Cabernet.from_file(filename_or_obj=parafac2_results_path)\n",
    "cab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = cab[\"parafac2/components\"]\n",
    "components.sel(mz=44).isel(sample=slice(0, 44, 5)).viz.line(\n",
    "    x=\"time\",\n",
    "    facet_dim=\"sample\",\n",
    "    overlay_dim=\"component\",\n",
    "    n_cols=3,\n",
    ").update_layout(\n",
    "    title=\"samples per component of the Zhang et. al demo interval\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "As we can see from the viz above, the decomposition is far from perfect, with some components capturing multiple peaks and many others capturing nothing. Furthermore there are multiple negative peaks, which is indicative of a very poor model. Further research will be required to explain these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "While I was able to successfully recreate Zhang's results and demonstrate that the scree test appears to be able to predict the number of components well when appropriate scaling is used. We found that CORCONDIA has random results which warrent more investigation, and that while PARAFAC performs admirably on a small number of peaks, it fails dramatically when applied to more complicated data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca-analysis-6KQS4gUX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
