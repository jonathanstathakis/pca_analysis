{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Rank Estimation of GC-MS Data\n",
    "description: A test of rank estimation methods on the Zhang et al. GC-MS data to recreate their results\n",
    "project: parafac2\n",
    "conclusion: was able to reproduce their results for the specified peaks, but the inclusion of more peaks resulted in failure\n",
    "status: closed\n",
    "cdt: 2024-08-30T00:00:00\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Replication of @zhang_flexibleimplementationtrilinearity_2022 Rank Estimation\n",
    "\n",
    "This study replicated the reported results of @zhang_flexibleimplementationtrilinearity_2022 on a Wine GC-MS dataset. The dataset is described in section 3.2 and the results in 4.2.\n",
    "\n",
    "We first recreated the dataset and developed methods to preprocess, unfold, decompose and display the estimated singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The GC-MS data was downloaded from [here](https://ucphchemometrics.com/2023/06/01/wine-samples-analyzed-by-gc-ms-and-ft-ir-instruments/) and originally prepared by @skov_multiblockvariancepartitioning_2008. It is stored in MATLAB 7.3 format and required the use of [mat73](https://gitlab.com/obob/pymatreader/) library. Within the library is GC-MS, FT-I and physicochemical univariate measurements. The GC-MS data consists of 44 samples x 2700 elution time-points and 200 mass channels.\n",
    "\n",
    "The authors narrowed the scope to range from 16.52 to 16.76 mins (corresponding to a range of 25 units) containing two compounds (peaks), described in detail in section 3.2. They identified three significant components (chemical rank) attributing two to the compounds stated and one to the background. We expect to find the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import numpy as np\n",
    "from pymatreader import read_mat\n",
    "import xarray as xr\n",
    "from pca_analysis.notebooks.experiments.zhang_pca_scree_plot import PCA\n",
    "from pca_analysis.definitions import DATA_DIR\n",
    "from pca_analysis import xr_plotly\n",
    "\n",
    "xr.set_options(display_expand_data=False)\n",
    "\n",
    "mat_path = DATA_DIR / \"Wine_v7.mat\"\n",
    "nc_path = DATA_DIR / \"Wine_v7.nc\"\n",
    "\n",
    "if nc_path.exists():\n",
    "    full_data = xr.open_dataarray(nc_path)\n",
    "\n",
    "else:\n",
    "    mat_data = read_mat(mat_path)\n",
    "\n",
    "    key_it = [\n",
    "        \"Label_Wine_samples\",\n",
    "        \"Label_Elution_time\",\n",
    "        \"Label_Mass_channels\",\n",
    "    ]\n",
    "\n",
    "    full_data = xr.DataArray(\n",
    "        mat_data[\"Data_GC\"],\n",
    "        dims=[\"sample\", \"time\", \"mz\"],\n",
    "        coords=[mat_data[k] for k in key_it],\n",
    "    )\n",
    "\n",
    "    full_data.name = \"zhang_data\"\n",
    "    full_data.to_netcdf(nc_path)\n",
    "\n",
    "\n",
    "# need number of time cells to be greater than the predicted rank (22)\n",
    "zhang_demo_intvl = full_data.sel(\n",
    "    time=slice(16.52 - 0.08, 16.76 + 0.08)\n",
    ")  # interval taken from article\n",
    "display(zhang_demo_intvl)\n",
    "\n",
    "(\n",
    "    zhang_demo_intvl.sel(mz=slice(0, 100))\n",
    "    .isel(sample=slice(0, 44, 4))\n",
    "    .plotly.facet(plot_type=\"heatmap\", x=\"mz\", y=\"time\", z=\"sample\", n_cols=3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The data interval is displayed above and corresponds to that shown by @zhang_flexibleimplementationtrilinearity_2022 figure 5. The 2 peaks are best viewed at mz = 44, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_demo_intvl.sel(mz=44).plotly.line(x=\"time\", color=\"sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    zhang_demo_intvl.isel(mz=slice(40, 60))\n",
    "    .isel(sample=slice(0, 44, 4))\n",
    "    .plotly.facet(n_cols=2, x=\"time\", y=\"mz\", z=\"sample\", plot_type=\"line\")\n",
    ")\n",
    "\n",
    "# .hvplot(by=\"sample\", alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Where we can see that the peaks are present in all samples at varying intensities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Unfolding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "@zhang_flexibleimplementationtrilinearity_2022 in section 2.5 describe the application of SVD (PCA) to different unfoldings of the data tensor. They state that depending on which mode is unfolded, the SVD will produce different numbers of significant components if the data is not perfectly trilinear, otherwise all three unfoldings will have the same number of significant components. They state that $X_{\\text{caug}}$ produces the most accurate estimate of significant components in the face of noise and trilinear disruption, assuming that each chemical species has a unique spectrum and their relative concentrations are independent. What is $X_{\\text{caug}}$? It is the unfolding $(I \\times K, J)$, which in the context of the dataset is $(\\text{retention times} \\times \\text{mz}, \\text{samples})$. Thus we first need to produce the augmented (unfolded) matrix $C_\\text{aug}$, unfolding along the sample mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_xarray.preprocessing import BaseTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def unfold(x):\n",
    "    return x.stack({\"aug\": (\"sample\", \"time\")}).transpose(..., \"mz\")\n",
    "\n",
    "\n",
    "class Unfolder(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        # required for API\n",
    "        self.groupby = None\n",
    "        pass\n",
    "\n",
    "    def _transform(self, X):\n",
    "        unfolded = unfold(X)\n",
    "        return unfolded\n",
    "\n",
    "\n",
    "pipeline = Pipeline([(\"unfold\", Unfolder()), (\"pca\", PCA())])\n",
    "pipeline.fit_transform(zhang_demo_intvl)\n",
    "pipeline.named_steps[\"pca\"].scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Estimating the number of Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "While @zhang_flexibleimplementationtrilinearity_2022 specify the use of the SVD, a useful interface for estimating the number of components is sklearn's `PCA`. A rudimentary scree plot can be used to observe the inflection point described in figure 3. The authors state in section 4.1 that when observing a function of the explained variance against the number of components, the point where the explained variance does not \"change much anymore\" is the point where the components start describing the noise of the dataset rather than chemical species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "And as we can see, we're able to recreate the results if the cutoff of the magnitude of change is set to 0.005."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Full Dataset\n",
    "\n",
    "Now we'll see what happens if the same method is applied to the full dataset, as its only valid if it works for any number of peaks. If not then some underlying mechanism is at work. To reiterate, we're expecting the number of significant components == the number of peaks == the number of unique chemical species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Visual Estimation of Number of Peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "What are the number of components expected? It should be close to the number of peaks in the maximum mass channel. This is because its a fair assumption that the sample with the most abundant chemical species is also the most intense. To find this value we will find the mass channel with the highest amplitude then the sample with the highest average amplitude at that mass channel. That sample and mz is displayed below, with its peaks highlighted in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average mz over time and samples\n",
    "\n",
    "mean_max_mz_label = full_data.mean(\"time\").mean(\"sample\").idxmax().pipe(int)\n",
    "\n",
    "max_sample_label = full_data.isel(mz=mean_max_mz_label).mean(\"time\").idxmax().item()\n",
    "\n",
    "max_sample = full_data.sel(sample=max_sample_label, mz=mean_max_mz_label)\n",
    "display(max_sample.sel(time=slice(4, 24)).plotly.line(x=\"time\"));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.peak_picking import compute_dataarray_peak_table\n",
    "\n",
    "\n",
    "pt = compute_dataarray_peak_table(\n",
    "    xa=zhang_demo_intvl.transpose(\"sample\", \"mz\", \"time\")\n",
    "    .isel(sample=slice(0, 5))\n",
    "    .sel(mz=slice(40, 48))\n",
    ")\n",
    "ds = xr.merge([pt, zhang_demo_intvl])\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.notebooks.experiments.zhang_experiment_find_peaks import FindPeaks\n",
    "\n",
    "fp = FindPeaks()\n",
    "fp.find_peaks(sample=max_sample)\n",
    "display(fp.plot_peaks(sample=max_sample))\n",
    "n_peaks = len(fp.peaks_x)\n",
    "\n",
    "display(f\"{n_peaks=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "TODO continue revising, cleaning, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(\n",
    "    f\"Visually it appears that all peaks are accounted for, thus the number of peaks is around {n_peaks}. Thus we should expect around {n_peaks} significant components through PCA.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Unfold Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit_transform(full_data)\n",
    "pca = pipeline.named_steps[\"pca\"]\n",
    "pca.scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "But as we can see, the profile is coincidentally similar to the 2 peak slice with the vast majority of the variance explained by the first three compoonents, We can presume that this is because of the dominance of the maxima peak. If we remove it from the set.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Full Dataset Without Maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_global_maxima(xa: xr.DataArray, peaks_x: np.ndarray, peaks_y: np.ndarray):\n",
    "    \"\"\"\n",
    "    select the subset of the input signal running from half way between the maxima and\n",
    "    the next peak until the end of the signal\n",
    "    \"\"\"\n",
    "    maxima_idx = peaks_y.argmax()\n",
    "    next_peak_idx = maxima_idx + 1\n",
    "\n",
    "    # maxima idx plus the next idx minus the maxima idx.\n",
    "    # the time half way between the two.\n",
    "    maxima_time = peaks_x[maxima_idx]\n",
    "    next_peak_time = peaks_x[next_peak_idx]\n",
    "\n",
    "    cut_time_start = maxima_time + ((next_peak_time - maxima_time) / 2)\n",
    "\n",
    "    without_global_maxima = xa.sel(time=slice(cut_time_start, None))\n",
    "\n",
    "    return without_global_maxima\n",
    "\n",
    "\n",
    "without_global_maxima = full_data.pipe(\n",
    "    exclude_global_maxima, peaks_x=fp.peaks_x, peaks_y=fp.peaks_y\n",
    ")\n",
    "without_global_maxima.pipe(display)\n",
    "without_global_maxima.sel(mz=44).isel(sample=slice(0, 44, 11)).plotly.line(\n",
    "    x=\"time\", facet_col=\"sample\", facet_col_wrap=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Looks good. Whats the PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"unfold\", Unfolder()), (\"pca\", PCA())])\n",
    "pipeline.fit_transform(without_global_maxima)\n",
    "pipeline.named_steps[\"pca\"].scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "An improvement, but evidently the PCA is still being dominated by a subset of latent variables. A possible solution is in standard scaling..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Scaled and Centered PCA Global Maxima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Scaling and centering can help models such as PCA fit noisy or otherwise abbhorant data. In this context, where after unfolding the dataset is represented by samples (sample and timewise rows) and features (spectral dimension columns), then centering is the subtraction of each columns mean from each row and scaling is the division of each by the columns standard variation. This is implemented by `sklearn`'s `StandardScaler`. The result is that each column ranges from 0 to 1 and has a mean of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_std_scl = Pipeline(\n",
    "    [\n",
    "        (\"unfold\", Unfolder()),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"pca\", PCA()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_with_std_scl.fit_transform(full_data)\n",
    "pca2 = pipeline_with_std_scl.named_steps[\"pca\"]\n",
    "pca2.scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "A reversal in results. It appears that PCA with this user defined metric estimates an optimal number of components equal to the number of peaks detected earlier. Evidently standard scaling the data meaningfully increases the number of significant components estimated by the scree plot. This indicates that it is an appropriate method of estimation for unaligned GC-MS data. To further explore the veracity of the result we would need to estimate the total number of unique peaks across the sample dimension, as this verification has only observed the sample with the absorbance maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## CORCONDIA\n",
    "\n",
    "CORCONDIA is a method of approximating the number of components of PARAFAC and PARAFAC-like models (i.e. PARAFAC2) through observation of a Tucker3 core [@bro_newefficientmethod_2003]. The algorithm iterates through components, starting at 1, until a user-specified limit, and we are looking for a steep dropoff as the indication of the optimal number of components. In the interest of speed we will restrict the signal to the 15 to 25 minute interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corcondia import corcondia_3d\n",
    "\n",
    "display(zhang_demo_intvl)\n",
    "display(\n",
    "    zhang_demo_intvl.sel(\n",
    "        mz=44,\n",
    "    )\n",
    "    .isel(sample=slice(0, 44, 11))\n",
    "    .plotly.line(x=\"time\", color=\"sample\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CORCONDIA:\n",
    "    def __init__(self, X):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise TypeError\n",
    "        self.X = X\n",
    "        self.diagnostics = None\n",
    "        self.ranks = None\n",
    "        pass\n",
    "\n",
    "    def compute_range(self, rank_range=(1, 2)):\n",
    "        \"\"\"\n",
    "        calculate the CORCONDIA diagonstic for a range of ranks\n",
    "        \"\"\"\n",
    "\n",
    "        self.diagnostics = []\n",
    "        self.ranks = []\n",
    "        for k in range(rank_range[0], rank_range[1] + 1):\n",
    "            self.ranks.append(k)\n",
    "            self.diagnostics.append(self.compute_diagnostic(rank=k))\n",
    "\n",
    "    def compute_diagnostic(self, rank):\n",
    "        \"\"\"\n",
    "        compute the diagonistic for a given rank\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(rank, int)\n",
    "        assert rank > 0\n",
    "        return corcondia_3d(X=self.X, k=rank)\n",
    "\n",
    "    def plot_diagonstic(self):\n",
    "        \"\"\"\n",
    "        viz the diagnostic range generated by `self.compute_range`\n",
    "        \"\"\"\n",
    "\n",
    "        if (self.diagnostics is None) or (self.ranks is None):\n",
    "            raise RuntimeError(\"run `compute_range` first\")\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=self.ranks, y=self.diagnostics))\n",
    "        fig.update_layout(\n",
    "            title=\"CORCONDIA Diagnostic vs. Model Rank\",\n",
    "            xaxis=dict(title=\"rank\"),\n",
    "            yaxis=dict(title=\"diagnostic\"),\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "\n",
    "cc = CORCONDIA(X=zhang_demo_intvl.to_numpy())\n",
    "cc.compute_range(rank_range=(1, 10))\n",
    "cc.plot_diagonstic()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "As we can see, the CORCONDIA results are ambiguous over a wide range but indicate that a rank of 3 maintains model stability, which is in agreement with the PCA results. Note that while we havent' demonstrated it here ,the results are semi-random and that deserves more study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## PARAFAC2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### PARAFAC2 Demo Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "A demonstration of PARAFAC2 on the demo dataset. We demonstrated earlier that a rank of 3 is appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "zhang_demo_intvl.sel(mz=44).plotly.line(x=\"time\", color=\"sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.parafac2_pipeline.estimators import PARAFAC2\n",
    "from pca_analysis import xr_plotly  # noqa\n",
    "\n",
    "rank_range = 3\n",
    "\n",
    "parafac2_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"parafac2\",\n",
    "            PARAFAC2(rank=rank_range, n_iter_max=500, nn_modes=\"all\", linesearch=False),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "parafac2_pipeline.fit_transform(zhang_demo_intvl)\n",
    "\n",
    "parafac2 = parafac2_pipeline.named_steps[\"parafac2\"]\n",
    "results = parafac2.results_as_xr()\n",
    "\n",
    "components = results.components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "components.sel(mz=44).transpose(\"component\", \"sample\", ...).plotly.facet(\n",
    "    x=\"time\", y=\"sample\", z=\"component\", n_cols=2\n",
    ").update_layout(\n",
    "    height=500, title=\"samples per component of the Zhang et. al demo interval\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "This result corresponds to the deconstruction shown in @zhang_flexibleimplementationtrilinearity_2022, Figure 6.\n",
    "\n",
    "TODO make viz better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### PARAFAC2 Full Dataset\n",
    "\n",
    "We demonstrated earlier that through the PCA approach, we estimate 22 components for the full dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The decomposition of the full dataset with rank = 22 and n_iter_max = 500 takes 7m 25s which is too slow to retain for testing purposes. Furthermore the results are disappointing, with some components capturing multiple peaks while others capture nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results from cache if the file is present, saving time.\n",
    "\n",
    "full_data_reults_path = DATA_DIR / \"zhang_full_data_parafac2_decomp_results.nc\"\n",
    "if not full_data_reults_path.exists():\n",
    "    parafac2_pipeline.set_params(parafac2__rank=22).fit_transform(full_data)\n",
    "    parafac2 = parafac2_pipeline.named_steps[\"parafac2\"]\n",
    "    results = parafac2.results_as_xr()\n",
    "\n",
    "    results.to_netcdf(full_data_reults_path, engine=\"netcdf4\")\n",
    "\n",
    "\n",
    "else:\n",
    "    results = xr.open_dataset(full_data_reults_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = (\n",
    "    results.components[0:6]\n",
    "    .sel(mz=44)\n",
    "    .isel(component=slice(0, 4))\n",
    "    .transpose(\"component\", \"sample\", ...)\n",
    ")\n",
    "xa.name = \"abs\"\n",
    "n_cols = 2\n",
    "x = \"time\"\n",
    "\n",
    "fig = xa.plotly.facet(x=\"time\", y=\"sample\", z=\"component\", n_cols=2)\n",
    "fig.update_layout(title=\"sample overlays per component\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "As we can see from the viz above, the decomposition is far from perfect, with some components capturing multiple peaks and many others capturing nothing. Furthermore there are multiple negative peaks, which is indicative of a very poor model. Further research will be required to explain these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "While I was able to successfully recreate Zhang's results and demonstrate that the scree test appears to be able to predict the number of components well when appropriate scaling is used. We found that CORCONDIA has random results which warrent more investigation, and that while PARAFAC performs admirably on a small number of peaks, it fails dramatically when applied to more complicated data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca-analysis-6KQS4gUX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
