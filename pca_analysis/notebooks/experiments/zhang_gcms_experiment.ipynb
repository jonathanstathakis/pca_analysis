{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Rank Estimation of GC-MS Data\n",
    "description: A test of rank estimation methods on the Zhang et al. GC-MS data to recreate their results\n",
    "project: parafac2\n",
    "conclusion: was able to reproduce their results for the specified peaks, but the inclusion of more peaks resulted in failure\n",
    "status: closed\n",
    "cdt: 2024-08-30T00:00:00\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Rank Estimation\n",
    "\n",
    "An experiment of rank estimation on a toy dataset and the raw UV-Vis dataset. It will follow the method described in @zhang_flexibleimplementationtrilinearity_2022. First we will recreate the GC-MS dataset as described and build an implementation to preprocess, unfold, decompose and display the estimated singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Toy Dataset\n",
    "\n",
    "The GC-MS data was downloaded from [here](https://ucphchemometrics.com/2023/06/01/wine-samples-analyzed-by-gc-ms-and-ft-ir-instruments/) and originally prepared by @skov_multiblockvariancepartitioning_2008. It is stored in MATLAB 7.3 format and required the use of [mat73](https://gitlab.com/obob/pymatreader/) library. Within the library is GC-MS, FT-I and physicochemical univariate measurements. The GC-MS data consists of 44 samples x 2700 elution time-points and 200 mass channels.\n",
    "\n",
    "The authors narrowed the scope to range from 16.52 to 16.76 mins (corresponding to a range of 25 units) containing two compounds (peaks). They identified three significant components (chemical rank) attributing two to the compounds stated and one to the background. We expect to find the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
"%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import numpy as np\n",
    "from pymatreader import read_mat\n",
    "import xarray as xr\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pca_analysis.definitions import DATA_DIR\n",
    "\n",
    "xr.set_options(display_expand_data=False)\n",
    "\n",
    "mat_path = DATA_DIR / \"Wine_v7.mat\"\n",
    "nc_path = DATA_DIR / \"Wine_v7.nc\"\n",
    "\n",
    "if nc_path.exists():\n",
    "    full_data = xr.open_dataarray(nc_path)\n",
    "\n",
    "else:\n",
    "    mat_data = read_mat(mat_path)\n",
    "\n",
    "    key_it = [\n",
    "        \"Label_Wine_samples\",\n",
    "        \"Label_Elution_time\",\n",
    "        \"Label_Mass_channels\",\n",
    "    ]\n",
    "\n",
    "    full_data = xr.DataArray(\n",
    "        mat_data[\"Data_GC\"],\n",
    "        dims=[\"sample\", \"time\", \"mz\"],\n",
    "        coords=[mat_data[k] for k in key_it],\n",
    "    )\n",
    "\n",
    "    full_data.to_netcdf(nc_path)\n",
    "\n",
    "display(full_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We will organise the data in a labelled xarray for ease of handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To perform the SVD the data needs to be scaled and centered. It doesn't appear as though @zhang_flexibleimplementationtrilinearity_2022 did this, at least they didnt report it. So I will begin PCA without. If the results are poor, I will integrate this preprocessing stage. I think I would mean center columns (observations) and scale rows (samples). TODO: find resources for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "I will use [tensorly](https://tensorly.org/stable/user_guide/tensor_basics.html#unfolding) to unfold the numpy array. Leaving the xarray for now as it has a potentially different API. Which is fine, but now the modes are unlabelled. Which is still fine, just need to reapply them. Easiest to extract then implement manually. Its probably easist just to find the indexes manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sliced_data = full_data.sel(time=slice(16.52, 16.76))  # interval taken from article\n",
    "display(sliced_data)\n",
    "sliced_data[0].plot.line(x=\"time\", add_legend=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Which looks correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Unfolding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3bb57f",
   "metadata": {},
   "source": [
    "Unfolding along sample mode such that the matrix is stacked sample-wise with rows as sample, time and columns as wavelengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c68030",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_aug = sliced_data.stack(sample_aug=(\"sample\", \"time\")).transpose(..., \"mz\")\n",
    "sample_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "As we are skipping scaling and centering for now, we will proceed with PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "sample_aug.pipe(pca.fit_transform);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We will crudely define the inflection point as being the location where the finite difference becomes less than 2% of the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.notebooks.experiments.zhang_pca_scree_plot import pca_scree_plot\n",
    "\n",
    "pca_scree_plot(explained_variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "user_expressions": [
     {
      "expression": "sig_component_idxs[0][-1]+1",
      "result": {
       "ename": "NameError",
       "evalue": "name 'sig_component_idxs' is not defined",
       "status": "error",
       "traceback": [
        "\u001b[0;31mNameError\u001b[0m\u001b[0;31m:\u001b[0m name 'sig_component_idxs' is not defined\n"
       ]
      }
     }
    ]
   },
   "source": [
    "According to the rule of <2% change, the number of significant components is deemed to be {eval}`sig_component_idxs[0][-1]+1`, which is in agreement with the authors, and no preprocessing was necessary.\n",
    "\n",
    "This example will remain as a test case, we will now test it on the entire toy dataset, then my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Full Dataset\n",
    "\n",
    "Do the same thing but for the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Visual Estimation of Number of Peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "What are the number of components expected? It should be close to the number of peaks in the maximum mass channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c096685",
   "metadata": {},
   "source": [
    "#### MZ Maxima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average mz over time and samples\n",
    "\n",
    "mean_max_mz_label = full_data.mean(\"time\").mean(\"sample\").idxmax().pipe(int)\n",
    "\n",
    "f\"the mz with the highest mean amplitude is {mean_max_mz_label}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "And which sample has the highest average for that channel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample_label = full_data.isel(mz=mean_max_mz_label).mean(\"time\").idxmax().item()\n",
    "f\"the sample with the highest mean amplitude at {mean_max_mz_label} is: {max_sample_label}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c269b",
   "metadata": {},
   "source": [
    "Whose chromatogram at 44 looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample = full_data.sel(sample=max_sample_label, mz=mean_max_mz_label)\n",
    "max_sample.plot.line();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.notebooks.experiments.zhang_experiment_find_peaks import FindPeaks\n",
    "\n",
    "\n",
    "fp = FindPeaks()\n",
    "fp.find_peaks(sample=max_sample)\n",
    "fp.plot_peaks(sample=max_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e04cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_peaks = len(fp.peaks_x)\n",
    "n_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Visually it appears that all peaks are accounted for, thus the number of peaks is around {n_peaks}. Thus we should expect around {n_peaks} significant components through PCA.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Unfold Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_aug_full = full_data.stack({\"aug\": (\"sample\", \"time\")}).transpose(..., \"mz\")\n",
    "sample_aug_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.notebooks.experiments.zhang_pca_scree_plot import MyPCA\n",
    "\n",
    "pca = MyPCA()\n",
    "pca = pca.run_pca(sample_aug_full)\n",
    "pca.scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "But as we can see, the profile is coincidentally similar to the 2 peak slice with the vast majority of the variance explained by the first three compoonents, infact by the first two. We can presume that this is because of the dominance of the maxima peak. If we remove it from the set.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Full Dataset Without Maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_global_maxima(xa: xr.DataArray, peaks_x: np.ndarray, peaks_y: np.ndarray):\n",
    "    \"\"\"\n",
    "    select the subset of the input signal running from half way between the maxima and\n",
    "    the next peak until the end of the signal\n",
    "    \"\"\"\n",
    "    maxima_idx = peaks_y.argmax()\n",
    "    next_peak_idx = maxima_idx + 1\n",
    "\n",
    "    # maxima idx plus the next idx minus the maxima idx.\n",
    "    # the time half way between the two.\n",
    "    maxima_time = peaks_x[maxima_idx]\n",
    "    next_peak_time = peaks_x[next_peak_idx]\n",
    "\n",
    "    cut_time_start = maxima_time + ((next_peak_time - maxima_time) / 2)\n",
    "\n",
    "    without_global_maxima = xa.sel(time=slice(cut_time_start, None))\n",
    "\n",
    "    return without_global_maxima\n",
    "\n",
    "\n",
    "without_global_maxima = exclude_global_maxima(\n",
    "    xa=full_data, peaks_x=fp.peaks_x, peaks_y=fp.peaks_y\n",
    ")\n",
    "display(without_global_maxima)\n",
    "without_global_maxima.sel(mz=44).isel(sample=slice(0, 44, 11)).plot(\n",
    "    col=\"sample\", col_wrap=2\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Looks good. Whats the PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    without_global_maxima.stack({\"aug\": (\"sample\", \"time\")})\n",
    "    .transpose(..., \"mz\")\n",
    "    .pipe(pca.run_pca)\n",
    "    .scree_plot()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "An improvement, but evidently the PCA is still being dominated by a subset of latent variables.\n",
    "\n",
    " Its time to implement scaling and centering, then if that doesnt work, binning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45910719",
   "metadata": {},
   "source": [
    "## Scaled and Centered PCA Global Maxima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Scaling and centering can help models such as PCA fit noisy or otherwise abbhorant data. In this context, where after unfolding the dataset is represented by samples (sample and timewise rows) and features (spectral dimension columns), then centering is the subtraction of each columns mean from each row and scaling is the division of each by the columns standard variation. This is implemented by `sklearn`'s `StandardScaler`. The result is that each column ranges from 0 to 1 and has a mean of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_xarray import wrap\n",
    "from sklearn_xarray.preprocessing import BaseTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def unfold(x):\n",
    "    return x.stack({\"aug\": (\"sample\", \"time\")}).transpose(..., \"mz\")\n",
    "\n",
    "\n",
    "class Unfolder(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        # required for API\n",
    "        self.groupby = None\n",
    "        pass\n",
    "\n",
    "    def _transform(self, X):\n",
    "        unfolded = unfold(X)\n",
    "        return unfolded\n",
    "\n",
    "\n",
    "prepro_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"unfold\", Unfolder()),\n",
    "        (\"scale\", wrap(StandardScaler, reshapes=\"feature\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Xt = prepro_pipeline.fit_transform(full_data)\n",
    "\n",
    "pca2 = MyPCA()\n",
    "(Xt.pipe(pca2.run_pca).scree_plot(exp_var_change_prop=0.01))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "A reversal in results. It appears that PCA with this user defined metric estimates an optimal number of components equal to the number of peaks detected earlier. Evidently standard scaling the data meaningfully increases the number of significant components estimated by the scree plot. This indicates that it is an appropriate method of estimation for unaligned GC-MS data. To further explore the veracity of the result we would need to estimate the total number of unique peaks across the sample dimension, as this verification has only observed the sample with the absorbance maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## CORCONDIA\n",
    "\n",
    "As the PCA approaches are failing to produce the expected results without extensive manual handling, we will move on to CORCONDIA, a method of approximating the number of components of PARAFAC and PARAFAC-like models (i.e. PARAFAC2) through observation of a Tucker3 core [@bro_newefficientmethod_2003]. It will iterate through components, starting at 1, until a limit (?) and we are looking for a steep dropoff as the indication of the optimal number of components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corcondia import corcondia_3d\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "corcondia_3d(X=raw_data.transpose(\"sample\", \"time\", \"mz\").to_numpy(), k=22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "corcondia hasnt worked. The vibe of things is that none of these tools work for high numbers of peaks. Binning is required.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## PARAFAC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.decomposition import parafac2\n",
    "\n",
    "best_err = np.inf\n",
    "decomposition = None\n",
    "\n",
    "true_rank = 22\n",
    "\n",
    "for run in range(1):\n",
    "    print(f\"Training model {run}...\")\n",
    "    trial_decomposition, trial_errs = parafac2(\n",
    "        raw_data[0:5, :, :].to_numpy(),\n",
    "        true_rank,\n",
    "        return_errors=True,\n",
    "        tol=1e-8,\n",
    "        n_iter_max=500,\n",
    "        random_state=run,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f\"Number of iterations: {len(trial_errs)}\")\n",
    "    print(f\"Final error: {trial_errs[-1]}\")\n",
    "    if best_err > trial_errs[-1]:\n",
    "        best_err = trial_errs[-1]\n",
    "        err = trial_errs\n",
    "        decomposition = trial_decomposition\n",
    "    print(\"-------------------------------\")\n",
    "print(f\"Best model error: {best_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
