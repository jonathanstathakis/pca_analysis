{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Rank Estimation of GC-MS Data\n",
    "description: A test of rank estimation methods on the Zhang et al. GC-MS data to recreate their results\n",
    "project: parafac2\n",
    "conclusion: was able to reproduce their results for the specified peaks, but the inclusion of more peaks resulted in failure\n",
    "status: closed\n",
    "cdt: 2024-08-30T00:00:00\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Rank Estimation\n",
    "\n",
    "An experiment of rank estimation on a toy dataset and the raw UV-Vis dataset. It will follow the method described in @zhang_flexibleimplementationtrilinearity_2022. First we will recreate the GC-MS dataset as described and build an implementation to preprocess, unfold, decompose and display the estimated singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Toy Dataset\n",
    "\n",
    "The GC-MS data was downloaded from [here](https://ucphchemometrics.com/2023/06/01/wine-samples-analyzed-by-gc-ms-and-ft-ir-instruments/) and originally prepared by @skov_multiblockvariancepartitioning_2008. It is stored in MATLAB 7.3 format and required the use of [mat73](https://gitlab.com/obob/pymatreader/) library. Within the library is GC-MS, FT-I and physicochemical univariate measurements. The GC-MS data consists of 44 samples x 2700 elution time-points and 200 mass channels.\n",
    "\n",
    "The authors narrowed the scope to range from 16.52 to 16.76 mins (corresponding to a range of 25 units) containing two compounds (peaks). They identified three significant components (chemical rank) attributing two to the compounds stated and one to the background. We expect to find the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
"%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "import numpy as np\n",
    "from pymatreader import read_mat\n",
    "import xarray as xr\n",
    "from tensorly import unfold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "from copy import deepcopy\n",
    "\n",
    "from pca_analysis.definitions import DATA_DIR, ROOT\n",
    "\n",
    "xr.set_options(display_expand_data=False)\n",
    "\n",
    "mat_path = DATA_DIR / \"Wine_v7.mat\"\n",
    "nc_path = DATA_DIR / \"Wine_v7.nc\"\n",
    "\n",
    "if nc_path.exists():\n",
    "    full_data = xr.open_dataarray(nc_path)\n",
    "\n",
    "else:\n",
    "    mat_data = read_mat(mat_path)\n",
    "\n",
    "    key_it = [\n",
    "        \"Label_Wine_samples\",\n",
    "        \"Label_Elution_time\",\n",
    "        \"Label_Mass_channels\",\n",
    "    ]\n",
    "\n",
    "    raw_data = xr.DataArray(\n",
    "        mat_data[\"Data_GC\"],\n",
    "        dims=[\"sample\", \"time\", \"mz\"],\n",
    "        coords=[mat_data[k] for k in key_it],\n",
    "    )\n",
    "\n",
    "    raw_data.to_netcdf(nc_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We will organise the data in a labelled xarray for ease of handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To perform the SVD the data needs to be scaled and centered. It doesn't appear as though @zhang_flexibleimplementationtrilinearity_2022 did this, at least they didnt report it. So I will begin PCA without. If the results are poor, I will integrate this preprocessing stage. I think I would mean center columns (observations) and scale rows (samples). TODO: find resources for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "I will use [tensorly](https://tensorly.org/stable/user_guide/tensor_basics.html#unfolding) to unfold the numpy array. Leaving the xarray for now as it has a potentially different API. Which is fine, but now the modes are unlabelled. Which is still fine, just need to reapply them. Easiest to extract then implement manually. Its probably easist just to find the indexes manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sliced_data = full_data.sel(time=slice(16.52, 16.76))  # interval taken from article\n",
    "display(sliced_data)\n",
    "sliced_data[0].plot.line(x=\"time\", add_legend=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Which looks correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Unfolding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3bb57f",
   "metadata": {},
   "source": [
    "Unfolding along sample mode such that the matrix is stacked sample-wise with rows as sample, time and columns as wavelengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c68030",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_aug = sliced_data.stack(sample_aug=(\"sample\", \"time\")).transpose(..., \"mz\")\n",
    "sample_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "As we are skipping scaling and centering for now, we will proceed with PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "sample_aug.pipe(pca.fit_transform);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We will crudely define the inflection point as being the location where the finite difference becomes less than 2% of the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.notebooks.experiments.zhang_pca_scree_plot import pca_scree_plot\n",
    "\n",
    "pca_scree_plot(explained_variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "user_expressions": [
     {
      "expression": "sig_component_idxs[0][-1]+1",
      "result": {
       "ename": "NameError",
       "evalue": "name 'sig_component_idxs' is not defined",
       "status": "error",
       "traceback": [
        "\u001b[0;31mNameError\u001b[0m\u001b[0;31m:\u001b[0m name 'sig_component_idxs' is not defined\n"
       ]
      }
     }
    ]
   },
   "source": [
    "According to the rule of <2% change, the number of significant components is deemed to be {eval}`sig_component_idxs[0][-1]+1`, which is in agreement with the authors, and no preprocessing was necessary.\n",
    "\n",
    "This example will remain as a test case, we will now test it on the entire toy dataset, then my dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Full Dataset\n",
    "\n",
    "Do the same thing but for the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Visual Estimation of Number of Peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "What are the number of components expected? It should be close to the number of peaks in the maximum mass channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c096685",
   "metadata": {},
   "source": [
    "#### MZ Maxima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average mz over time and samples\n",
    "\n",
    "mean_max_mz = int(full_data.mean(\"time\").mean(\"sample\").idxmax().item())\n",
    "f\"the mz with the highest mean amplitude is {mean_max_mz}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "And which sample has the highest average for that channel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample = full_data.sel({\"mz\": mean_max_mz}).mean(\"time\").idxmax().item()\n",
    "f\"the sample with the highest mean amplitude at {mean_max_mz} is: {max_sample}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c269b",
   "metadata": {},
   "source": [
    "Whose chromatogram at 44 looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample = full_data.sel({\"mz\": mean_max_mz, \"sample\": max_sample})\n",
    "max_sample.plot.line()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.notebooks.experiments.zhang_experiment_find_peaks import FindPeaks\n",
    "\n",
    "\n",
    "fp = FindPeaks()\n",
    "fp.find_peaks(sample=max_sample)\n",
    "fp.plot_peaks(sample=max_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e04cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_peaks = len(fp.peaks_x)\n",
    "n_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Visually it appears that all peaks are accounted for, thus the number of peaks is around {n_peaks}. Thus we should expect around {n_peaks} significant components through PCA.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Unfold Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_aug_full = full_data.stack({\"aug\": (\"sample\", \"time\")}).transpose(..., \"mz\")\n",
    "sample_aug_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pca_analysis.notebooks.experiments.zhang_pca_scree_plot import MyPCA\n",
    "\n",
    "\n",
    "pca = MyPCA()\n",
    "pca = pca.run_pca(sample_aug_full)\n",
    "pca.scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "But as we can see, the profile is coincidentally similar to the 2 peak slice with the vast majority of the variance explained by the first three compoonents, infact by the first two. We can presume that this is because of the dominance of the maxima peak. If we remove it from the set.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Without Maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut it the signal half way between the first two peaks\n",
    "\n",
    "# cut the maxima off while retaining a decent amount of baseline\n",
    "\n",
    "maxima_idx = peaks_y.argmax()\n",
    "next_peak_idx = maxima_idx + 1\n",
    "cut_time_start = peaks_x[maxima_idx] + (peaks_x[next_peak_idx] - peaks_x[maxima_idx])\n",
    "\n",
    "mean_distance = int(np.ceil(np.diff(peaks_x).mean()))\n",
    "last_peak = peaks_x[-1]\n",
    "cut_time_end = last_peak + mean_distance\n",
    "\n",
    "# chop the empty component of the signal off - empty is defined as a distance from the last peak equal to the average gap between peaks\n",
    "\n",
    "# average gap between peaks\n",
    "\n",
    "shortened = raw_data.where(\n",
    "    (raw_data.time >= cut_time_start) & (raw_data.time <= cut_time_end)\n",
    ").dropna(\"time\")\n",
    "shortened.sel(mz=44).plot(col=\"sample\", col_wrap=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Looks good. Whats the PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_shortened = shortened.stack({\"aug\": (\"sample\", \"time\")}).transpose(..., \"mz\")\n",
    "\n",
    "pca_shortened = pca.run_pca(data=stacked_shortened)\n",
    "pca_shortened.scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "um. Kind of better? At least the drop off isnt so harsh, but evidently the PCA is still being dominated by only a few latent variables. Its time to implement scaling and centering, then if that doesnt work, binning *shudder*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Want to mean center the columns, scale the sample rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "scaler = StandardScaler()\n",
    "normed = normalizer.fit_transform(stacked_shortened)\n",
    "scaled_normed = scaler.fit_transform(normed)\n",
    "scaled_normed.shape\n",
    "normed[0:10, 0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is mean centered if the following equals zero.\n",
    "\n",
    "np.mean(np.abs(np.round(np.mean(scaled_normed, axis=0), 9)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sn = pca.run_pca(scaled_normed)\n",
    "pca.scree_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Same. Fairly evident that binning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## CORCONDIA\n",
    "\n",
    "As the PCA approaches are failing to produce the expected results without extensive manual handling, we will move on to CORCONDIA, a method of approximating the number of components of PARAFAC and PARAFAC-like models (i.e. PARAFAC2) through observation of a Tucker3 core [@bro_newefficientmethod_2003]. It will iterate through components, starting at 1, until a limit (?) and we are looking for a steep dropoff as the indication of the optimal number of components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corcondia import corcondia_3d\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "corcondia_3d(X=raw_data.transpose(\"sample\", \"time\", \"mz\").to_numpy(), k=22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "corcondia hasnt worked. The vibe of things is that none of these tools work for high numbers of peaks. Binning is required.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## PARAFAC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.decomposition import parafac2\n",
    "\n",
    "best_err = np.inf\n",
    "decomposition = None\n",
    "\n",
    "true_rank = 22\n",
    "\n",
    "for run in range(1):\n",
    "    print(f\"Training model {run}...\")\n",
    "    trial_decomposition, trial_errs = parafac2(\n",
    "        raw_data[0:5, :, :].to_numpy(),\n",
    "        true_rank,\n",
    "        return_errors=True,\n",
    "        tol=1e-8,\n",
    "        n_iter_max=500,\n",
    "        random_state=run,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f\"Number of iterations: {len(trial_errs)}\")\n",
    "    print(f\"Final error: {trial_errs[-1]}\")\n",
    "    if best_err > trial_errs[-1]:\n",
    "        best_err = trial_errs[-1]\n",
    "        err = trial_errs\n",
    "        decomposition = trial_decomposition\n",
    "    print(\"-------------------------------\")\n",
    "print(f\"Best model error: {best_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
